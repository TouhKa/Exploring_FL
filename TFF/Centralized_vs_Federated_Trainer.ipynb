{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM4kVhzbR_UK"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow-federated\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbo1O-krQDQh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, callbacks\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow_model_optimization.python.core.internal import tensor_encoding as te\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import random\n",
    "import time\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import mlflow \n",
    "\n",
    "from Reader import Reader\n",
    "from model.FLModel import FLModel\n",
    "from model.BNModel import BNModel\n",
    "from Utils import Utils\n",
    "from TFF_Utils import TFF_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centralized_vs_Federated_Trainer:\n",
    "    \n",
    "    def __init__(self, file_name, number_of_classes):\n",
    "        self.file = file_name\n",
    "        self.NUM_CLASSES = number_of_classes\n",
    "        self.BASE_DIR = \"../\"\n",
    "        self.DATA_DIR = \"data/\"\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.PREFETCH_BUFFER = 10\n",
    "        self.SHUFFLE_BUFFER = 64\n",
    "        self.CLIENTS = 10\n",
    "\n",
    "        self.EPOCHS = 10 \n",
    "        self.E = 1 # amount of local rounds       \n",
    "        self.drop_index = True\n",
    "        \n",
    "        if \"infected\" in self.file:\n",
    "            self.drop_index = False\n",
    "            self.start_id = 1\n",
    "            self.label_id = 9\n",
    "        elif (\"Cosphere\" in self.file):\n",
    "            self.start_id = 1\n",
    "            self.label_id = 4\n",
    "        else:\n",
    "            self.start_id = 1\n",
    "            self.label_id = 3\n",
    "\n",
    "        self.k = 1\n",
    "        self.use_bn = False\n",
    "        self.use_tff = True\n",
    "        self.learning_rate = 1e-1\n",
    "        self.momentum = 0.9\n",
    "        self.nesterov = False\n",
    "        \n",
    "    \n",
    "    #mlflow setup\n",
    "    def init_mlflow(self):\n",
    "        mlflow.set_experiment(self.file)\n",
    "        mlflow.end_run()\n",
    "        mlflow.start_run()\n",
    "        mlflow_experiment_id = mlflow.get_experiment_by_name(self.file).experiment_id\n",
    "        mlflow_run_id = mlflow.active_run().info.run_id\n",
    "        log_path = \"mlruns/\" + str(mlflow_experiment_id) + \"/\" + str(mlflow_run_id) + \"/\" + \"artifacts\" + \"/\"\n",
    "\n",
    "        mlflow.log_param(\"local_rounds\", self.k)\n",
    "        mlflow.log_param(\"use_bn\", self.use_bn)\n",
    "        mlflow.log_param(\"use_tff\", self.use_tff)\n",
    "        if self.compression:\n",
    "            mlflow.log_param(\"use_compression\", self.compression)\n",
    "            mlflow.log_param(\"quant_bits\", self.quantization_bits)\n",
    "\n",
    "        mlflow.log_param(\"lr\", self.learning_rate)\n",
    "        mlflow.log_param(\"momentum\", self.momentum)\n",
    "        mlflow.log_param(\"nesterov\", self.nesterov)\n",
    "        mlflow.log_param(\"batch_size\", self.BATCH_SIZE)\n",
    "        mlflow.log_param(\"clients\", self.CLIENTS)\n",
    "        mlflow.log_param(\"epochs\", self.EPOCHS)\n",
    "        mlflow.log_param(\"classes\", self.NUM_CLASSES)\n",
    "            \n",
    "    def split_input_target(self, input, target):\n",
    "        return input, target\n",
    "\n",
    "    def create_dataset(self, x, y, use_tff = True):\n",
    "        ds =  tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "        if use_tff:\n",
    "            return (\n",
    "            ds.repeat(self.EPOCHS).shuffle(self.SHUFFLE_BUFFER)\n",
    "            .map(self.split_input_target)).batch(self.BATCH_SIZE) \n",
    "        else:\n",
    "            return ds.repeat(self.BATCH_SIZE * self.EPOCHS).shuffle(self.SHUFFLE_BUFFER).batch(self.BATCH_SIZE,drop_remainder = True) \n",
    "\n",
    "    def get_split(self, x, y):\n",
    "        return train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    def create_unfederated_dataset(self, x, features):\n",
    "        former_shape = x[:, self.start_id:features].shape\n",
    "        client_x = np.delete( x[:, self.start_id:features], self.label_id-1, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
    "        client_x = self.scaler.transform(client_x)\n",
    "        client_y = x[:, self.label_id].reshape(-1, 1)\n",
    "        X_train, X_test, y_train, y_test = self.get_split(client_x, client_y)\n",
    "        X_train, X_val, y_train, y_val = self.get_split(X_train, y_train)\n",
    "        train_data = self.create_dataset(X_train, y_train, use_tff=False)\n",
    "        test_data = self.create_dataset(X_test, y_test, use_tff=False)\n",
    "        val_data = self.create_dataset(X_val, y_val, use_tff=False)\n",
    "        return train_data, test_data, val_data\n",
    "    \n",
    "    def get_not_bn_idx(self, trainable_variables):\n",
    "      new_trainable_weights = []\n",
    "      train_var_idx = []\n",
    "      for idx, bn_weights in enumerate(trainable_variables):\n",
    "        if 'batch_normalization' not in bn_weights.name:\n",
    "          train_var_idx.append(idx)\n",
    "      return train_var_idx\n",
    "\n",
    "    def get_bn_idx(self, trainable_variables):\n",
    "      new_trainable_weights = []\n",
    "      train_var_idx = []\n",
    "      for idx, bn_weights in enumerate(trainable_variables):\n",
    "        if 'batch_normalization' in bn_weights.name:\n",
    "          train_var_idx.append(idx)\n",
    "      return train_var_idx\n",
    "\n",
    "    def get_weights_by_idx(self, trainable_variables, var_ids):\n",
    "      new_weights = []  \n",
    "      for i in var_ids:\n",
    "        new_weights.append(trainable_variables[i])\n",
    "      return new_weights\n",
    "\n",
    "    def create_keras_model(self, input_dim):\n",
    "        return tf.keras.models.Sequential([\n",
    "          tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "          tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "          tf.keras.layers.Dense(self.NUM_CLASSES, activation='softmax'),\n",
    "        ])\n",
    "\n",
    "    def create_keras_bn_model(self, input_dim):\n",
    "        return tf.keras.models.Sequential([\n",
    "          tf.keras.layers.BatchNormalization(input_shape=(input_dim,)),\n",
    "          tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "          tf.keras.layers.BatchNormalization(),\n",
    "          tf.keras.layers.Dense(self.NUM_CLASSES, activation='softmax'),\n",
    "        ])\n",
    "\n",
    "    # Each time the next method is called, the server model is broadcast to each client using a broadcast function. \n",
    "    # For each client, one epoch of local training is performed via the tf.keras.optimizers.Optimizer.apply_gradients method of the client optimizer. \n",
    "    # Each client computes the difference between the client model after training and the initial broadcast model. \n",
    "    # These model deltas are then aggregated at the server using some aggregation function. \n",
    "    # The aggregate model delta is applied at the server by using the tf.keras.optimizers.Optimizer.apply_gradients method of the server optimizer.\n",
    "    def model_fn(self):\n",
    "      # We _must_ create a new model here, and _not_ capture it from an external\n",
    "      # scope. TFF will call this within different graph contexts.\n",
    "        if self.use_bn:\n",
    "          print(\"Using model with batch normalization\")\n",
    "          keras_model = self.create_keras_bn_model(self.test_data[0].element_spec[0].shape[1])\n",
    "        else:\n",
    "          print(\"Using model without batch normalization\")\n",
    "          keras_model = self.create_keras_model(self.test_data[0].element_spec[0].shape[1])\n",
    "        return tff.learning.from_keras_model(\n",
    "          keras_model,\n",
    "          input_spec = self.train_data[0].element_spec,\n",
    "          loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "          metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "                   tf.keras.metrics.SparseTopKCategoricalAccuracy(k=self.k)]) \n",
    "\n",
    "\n",
    "    def map_weights_ids(self):\n",
    "      model = self.create_keras_bn_model(self.train_data[0].element_spec[0].shape[1])\n",
    "      trainable_variables = model.trainable_variables\n",
    "      all_weights_name = []\n",
    "      train_weights_ids = []\n",
    "\n",
    "      for layer in model.layers:\n",
    "        for weight in layer.weights:\n",
    "          all_weights_name.append(weight.name)\n",
    "\n",
    "      for var in trainable_variables:\n",
    "        for idx, name in enumerate(all_weights_name):\n",
    "          if var.name == name:\n",
    "            if 'batch_normalization' not in name:\n",
    "              train_weights_ids.append(idx)\n",
    "\n",
    "      return train_weights_ids\n",
    "\n",
    "    def broadcast_encoder_fn(self, value):\n",
    "      \"\"\"Function for building encoded broadcast.\"\"\"\n",
    "      spec = tf.TensorSpec(value.shape, value.dtype)\n",
    "      if value.shape.num_elements() > self.quantization_thresh:\n",
    "        return te.encoders.as_simple_encoder(\n",
    "            te.encoders.uniform_quantization(bits=self.quantization_bits), spec)\n",
    "      else:\n",
    "        return te.encoders.as_simple_encoder(te.encoders.identity(), spec)\n",
    "\n",
    "\n",
    "    def mean_encoder_fn(self, tensor_spec):\n",
    "      \"\"\"Function for building a GatherEncoder.\"\"\"\n",
    "      spec = tf.TensorSpec(tensor_spec.shape, tensor_spec.dtype)\n",
    "      if tensor_spec.shape.num_elements() > self.quantization_thresh:\n",
    "        return te.encoders.as_gather_encoder(\n",
    "            te.encoders.uniform_quantization(bits= self.quantization_bits), spec)\n",
    "      else:\n",
    "        return te.encoders.as_gather_encoder(te.encoders.identity(), spec)\n",
    "\n",
    "    def run_federated(self):\n",
    "        broadcast_process = None\n",
    "        mean_factory = None\n",
    "        \n",
    "        if self.compression: \n",
    "            # Equivalent to:\n",
    "            # compressed_mean = tff.learning.compression_aggregator(zeroing=False, clipping=False)\n",
    "            \n",
    "            # TODO try out Quantization bits = 6 or 7 bits\n",
    "            # If resources permit doing a small grid search, we would recommend that you identify the value \n",
    "            # for which training becomes unstable or final model quality starts to degrade, and then increase that value by two\n",
    "            \n",
    "            # TODO Clients per round. Note that significantly increasing the number of clients per round can enable a smaller value for quantization_bits to work well, \n",
    "            # because the randomized inaccuracy introduced by quantization may be evened out by averaging over more client updates\n",
    "            mean_factory = tff.aggregators.MeanFactory(\n",
    "                value_sum_factory = tff.aggregators.EncodedSumFactory(self.mean_encoder_fn),\n",
    "                weight_sum_factory = tff.aggregators.EncodedSumFactory(self.mean_encoder_fn)\n",
    "                )   \n",
    "            encoded_broadcast_process = (tff.learning.framework.build_encoded_broadcast_process_from_model(\n",
    "                                         self.model_fn, self.broadcast_encoder_fn))\n",
    "            \n",
    "\n",
    "        # ---custom fed avg implementation for batch normalization-----\n",
    "        # --------------------------START------------------------------\n",
    "        @tff.tf_computation\n",
    "        def server_init():\n",
    "            model = self.model_fn()\n",
    "            trainable_variables = model.trainable_variables\n",
    "            non_bn_ids = self.get_not_bn_idx(trainable_variables)\n",
    "            non_bn_weights = self.get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "            return non_bn_weights\n",
    "\n",
    "        @tf.function\n",
    "        def server_update(model, mean_client_weights):\n",
    "            \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
    "            updated_model_weights = []\n",
    "            trainable_variables = model.trainable_variables\n",
    "\n",
    "            non_bn_ids = self.get_not_bn_idx(trainable_variables)\n",
    "            bn_ids = self.get_bn_idx(trainable_variables)\n",
    "            non_bn_weights = self.get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "            bn_weights = self.get_weights_by_idx(trainable_variables, bn_ids)\n",
    "\n",
    "            tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                                    non_bn_weights, mean_client_weights)\n",
    "            for i in range(len(trainable_variables)):\n",
    "                if i in non_bn_ids:\n",
    "                    j = non_bn_ids.index(i)\n",
    "                    updated_model_weights.append(non_bn_weights[j])\n",
    "                else: \n",
    "                    j = bn_ids.index(i)\n",
    "                    updated_model_weights.append(bn_weights[j])\n",
    "            return non_bn_weights\n",
    "\n",
    "\n",
    "        @tf.function\n",
    "        def client_update(model, dataset, server_weights, client_optimizer):\n",
    "            updated_clients_weights = []\n",
    "            trainable_variables = model.trainable_variables\n",
    "            \n",
    "            #get ids of non_batch normalization weights\n",
    "            non_bn_ids = self.get_not_bn_idx(trainable_variables)\n",
    "            mlflow.log_param(\"number of updatable non-BN weights\", len(non_bn_ids))\n",
    "            \n",
    "            #get ids of batch normalization weights\n",
    "            bn_ids = self.get_bn_idx(trainable_variables)\n",
    "            \n",
    "            non_bn_weights = self.get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "            bn_weights = self.get_weights_by_idx(trainable_variables, bn_ids)\n",
    "            \n",
    "            # Assign the mean client weights to the server model.\n",
    "            tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                                    non_bn_weights, server_weights)\n",
    "\n",
    "            #update any weight which is not from a bn layer\n",
    "            for i in range(len(trainable_variables)):\n",
    "                if i in non_bn_ids:\n",
    "                    j = non_bn_ids.index(i)\n",
    "                    updated_clients_weights.append(non_bn_weights[j])\n",
    "                    \n",
    "                    #log shape of weight \n",
    "                    mlflow.log_param(f\"{j} Shape of updatable non-BN weight\", non_bn_weights[j].shape)\n",
    "                else: \n",
    "                    j = bn_ids.index(i)\n",
    "                    updated_clients_weights.append(bn_weights[j])\n",
    "\n",
    "            client_weights = updated_clients_weights\n",
    "\n",
    "            for epoch in range(self.E):\n",
    "                for batch in dataset:\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        outputs = model.forward_pass(batch)\n",
    "                    grads = tape.gradient(outputs.loss, client_weights)\n",
    "                    grads_and_vars = zip(grads, client_weights)\n",
    "                    client_optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "            return non_bn_weights\n",
    "\n",
    "        whimsy_model = self.model_fn()\n",
    "        tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)\n",
    "        model_weights_type = server_init.type_signature.result\n",
    "        federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
    "        federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
    "        print(federated_server_type)\n",
    "        print(federated_dataset_type)\n",
    "\n",
    "        @tff.tf_computation(tf_dataset_type, model_weights_type)\n",
    "        def client_update_fn(tf_dataset, server_weights):\n",
    "            model = self.model_fn()\n",
    "            client_optimizer = tf.keras.optimizers.SGD(learning_rate= self.learning_rate, momentum=self.momentum, nesterov=self.nesterov)\n",
    "            return client_update(model, tf_dataset, server_weights, client_optimizer)\n",
    "\n",
    "        @tff.tf_computation(model_weights_type)\n",
    "        def server_update_fn(mean_client_weights):\n",
    "            model = self.model_fn()\n",
    "            return server_update(model, mean_client_weights)\n",
    "\n",
    "        @tff.federated_computation\n",
    "        def initialize_fn():\n",
    "            return tff.federated_value(server_init(), tff.SERVER)\n",
    "\n",
    "\n",
    "        @tff.federated_computation(federated_server_type, federated_dataset_type)\n",
    "        def next_fn(server_weights, federated_dataset):\n",
    "\n",
    "        # Broadcast the server weights to the clients.\n",
    "            server_weights_at_client = tff.federated_broadcast(server_weights)\n",
    "\n",
    "            # Each client computes their updated weights.\n",
    "            client_weights = tff.federated_map(\n",
    "                client_update_fn, (federated_dataset, server_weights_at_client))\n",
    "\n",
    "            # The server averages these updates.\n",
    "            mean_client_weights = tff.federated_mean(client_weights)\n",
    "\n",
    "            # # The server updates its model.\n",
    "            server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
    "            return server_weights\n",
    "\n",
    "        def evaluate(server_state, train_data, test_data, trainable_ids):\n",
    "            server_weights = []\n",
    "            acc_mean, loss_mean, k_acc_mean = [], [], []\n",
    "            model = self.create_keras_bn_model(test_data[0].element_spec[0].shape[1])\n",
    "            model.compile(\n",
    "                    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "                            tf.keras.metrics.SparseTopKCategoricalAccuracy(k=self.k)]  \n",
    "            )\n",
    "            weights = model.get_weights()\n",
    "\n",
    "            for idx, weight_id in enumerate(trainable_ids):\n",
    "                weights[weight_id] = np.array(server_state[idx])\n",
    "\n",
    "            model.set_weights(weights)\n",
    "            print(\"\\t--Training--\\t\")\n",
    "\n",
    "            for batch in self.train_data:\n",
    "                loss, acc, k_acc = model.evaluate(batch, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "                loss_mean.append(loss)\n",
    "                acc_mean.append(acc)\n",
    "                k_acc_mean.append(k_acc)\n",
    "            train_loss = statistics.mean(loss_mean)\n",
    "            train_acc = statistics.mean(acc_mean)\n",
    "            train_k_acc = statistics.mean(k_acc_mean)\n",
    "            acc_mean, loss_mean, k_acc_mean = [], [], []\n",
    "\n",
    "            print(\"\\t--Testing--\\t\")\n",
    "            for batch in self.test_data:\n",
    "                loss, acc, top_k_acc = model.evaluate(batch, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "                loss_mean.append(loss)\n",
    "                acc_mean.append(acc)\n",
    "                k_acc_mean.append(k_acc)\n",
    "            test_loss = statistics.mean(loss_mean)\n",
    "            test_acc = statistics.mean(acc_mean)\n",
    "            test_k_acc = statistics.mean(k_acc_mean)\n",
    "            return {\"Train_acc\":train_acc, \n",
    "                    f\"Train_{self.k}_acc\":train_k_acc, \n",
    "                    \"Train_loss\":train_loss, \n",
    "                    \"Test_acc\":test_acc, \n",
    "                    f\"Test_{self.k}_acc\":test_k_acc,\n",
    "                    \"Test_loss\":test_loss,}\n",
    "        # -------------------END---------------------\n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "            if self.use_bn:    #if batch normalization dont update the clients weights of the batch normalization layer. \n",
    "                #returns just server state-> \n",
    "                trainable_ids = self.map_weights_ids()\n",
    "                fed_avg = tff.templates.IterativeProcess(\n",
    "                                        initialize_fn=initialize_fn,\n",
    "                                        next_fn=next_fn\n",
    "                                        )\n",
    "\n",
    "                state = fed_avg.initialize()\n",
    "                #-----\n",
    "                environment = self.tff_utils.set_sizing_environment()\n",
    "                #-----\n",
    "\n",
    "                for round_num in range(self.EPOCHS):  \n",
    "                    state = fed_avg.next(state, self.train_data)\n",
    "                  #---\n",
    "                    size_info = environment.get_size_info()\n",
    "                    broadcasted_bits = size_info.broadcast_bits[-1]\n",
    "                    aggregated_bits = size_info.aggregate_bits[-1]\n",
    "                    mlflow.log_metric('cumulative_broadcasted_bits', broadcasted_bits, round_num)\n",
    "                    mlflow.log_metric('cumulative_aggregated_bits', aggregated_bits, round_num)\n",
    "                    \n",
    "                    print('round {:2d}, broadcasted_bits={}, aggregated_bits={}'\n",
    "                          .format(round_num,  \n",
    "                                  self.tff_utils.format_size(broadcasted_bits), \n",
    "                                  self.tff_utils.format_size(aggregated_bits)\n",
    "                            )\n",
    "                         )\n",
    "                    #---\n",
    "                    \n",
    "                    metrics = evaluate(state, self.train_data, self.test_data, trainable_ids)\n",
    "                    for name, value in metrics.items():\n",
    "                        mlflow.log_metric(name, value, round_num)\n",
    "                        print(round_num, name, value)\n",
    "\n",
    "            else:\n",
    "                fed_avg = tff.learning.build_federated_averaging_process(\n",
    "                    self.model_fn,\n",
    "                    client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= self.learning_rate, momentum=self.momentum, nesterov=self.nesterov), \n",
    "                    server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= 1.0),\n",
    "                    broadcast_process = broadcast_process,\n",
    "                    model_update_aggregation_factory = mean_factory\n",
    "                    )\n",
    "\n",
    "                state = fed_avg.initialize()\n",
    "                \n",
    "                #-----\n",
    "                environment = self.tff_utils.set_sizing_environment()\n",
    "                #-----\n",
    "                \n",
    "                for round_num in range(self.EPOCHS):\n",
    "                    print(round_num)\n",
    "                    state, metrics = fed_avg.next(state, self.train_data)\n",
    "                    # Note: training metrics reported by the iterative training process \n",
    "                    #generally reflect the performance of the model at the beginning of the training round\n",
    "                    \n",
    "                    #---\n",
    "                    size_info = environment.get_size_info()\n",
    "                    broadcasted_bits = size_info.broadcast_bits[-1]\n",
    "                    aggregated_bits = size_info.aggregate_bits[-1]\n",
    "                    mlflow.log_metric('cumulative_broadcasted_bits', broadcasted_bits, round_num)\n",
    "                    mlflow.log_metric('cumulative_aggregated_bits', aggregated_bits, round_num)\n",
    "                    \n",
    "                    print('round {:2d}, metrics={}, broadcasted_bits={}, aggregated_bits={}'\n",
    "                          .format(round_num, metrics, \n",
    "                                  self.tff_utils.format_size(broadcasted_bits), \n",
    "                                  self.tff_utils.format_size(aggregated_bits)\n",
    "                            )\n",
    "                         )\n",
    "                    #---\n",
    "                    \n",
    "                    for name, value in metrics['train'].items():\n",
    "                        mlflow.log_metric(name, value, round_num)\n",
    "                        print(round_num, name, value)\n",
    "\n",
    "                    evaluation = tff.learning.build_federated_evaluation(self.model_fn)  \n",
    "                    test_metrics = evaluation(state.model, self.test_data)\n",
    "                    for name, value in test_metrics.items():\n",
    "                        mlflow.log_metric(name, value, round_num)\n",
    "                        print(round_num, name, value)\n",
    "\n",
    "    def run_unfederated(self, ds_train, ds_test, ds_val, input_dim):\n",
    "\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                    min_delta=0.01, \n",
    "                                    patience=2, \n",
    "                                    verbose=0, \n",
    "                                    mode='auto', \n",
    "                                    baseline=None, \n",
    "                                    restore_best_weights=True)\n",
    "        if self.use_bn:\n",
    "          model = BNModel(self.NUM_CLASSES)\n",
    "        else:\n",
    "          model = FLModel(self.NUM_CLASSES)\n",
    "        model.compile(\n",
    "                    optimizer= self.client_optimizer, \n",
    "                    loss= \"sparse_categorical_crossentropy\", \n",
    "                    metrics= [\n",
    "                              self.sparseCategoricalAcc, \n",
    "                              self.sparseTopKCategoricalAccuracy\n",
    "                              ]\n",
    "                    )\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            with tf.device('/gpu:0'):\n",
    "                history = model.fit(\n",
    "                                  ds_train,\n",
    "                                  steps_per_epoch=64, \n",
    "                                  validation_data = ds_val, \n",
    "                                  verbose=0,\n",
    "                                  callbacks = [early_stopping_callback])     \n",
    "\n",
    "            loss = round(history.history[\"loss\"][0], 8)\n",
    "            acc = round(history.history[\"sparse_categorical_accuracy\"][0], 8)\n",
    "            k_acc =  round(history.history[\"sparse_top_k_categorical_accuracy\"][0], 8)\n",
    "            val_loss =  round(history.history['val_loss'][0], 8)\n",
    "            val_acc = round(history.history['val_sparse_categorical_accuracy'][0], 8)\n",
    "            val_k_acc =  round(history.history['val_sparse_top_k_categorical_accuracy'][0], 8)\n",
    "                    \n",
    "            with tf.device('/gpu:0'):\n",
    "                    test_loss, test_acc, test_k_acc = model.evaluate(ds_test, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "\n",
    "            mlflow.log_metric(\"Loss/train\", loss, epoch)\n",
    "            mlflow.log_metric(\"Acc/train\", acc, epoch)\n",
    "            mlflow.log_metric(\"K_acc/train\", k_acc, epoch)\n",
    "\n",
    "            mlflow.log_metric(\"Loss/validation\", val_loss, epoch)\n",
    "            mlflow.log_metric(\"Acc/validation\", val_acc, epoch)\n",
    "            mlflow.log_metric(\"K_acc/validation\", val_k_acc, epoch)\n",
    "\n",
    "            mlflow.log_metric(\"Loss/test\", test_loss, epoch)\n",
    "            mlflow.log_metric(\"Acc/test\", test_acc, epoch)\n",
    "            mlflow.log_metric(\"K_acc/test\", test_k_acc, epoch)\n",
    "\n",
    "            print(\n",
    "              f'Epoch: {epoch},\\n'\n",
    "              f'Train Loss:\\t{loss}, '\n",
    "              f'Train Accuracy:\\t{acc}, '\n",
    "              f'Train Top 5 Accuracy:\\t{k_acc}\\n'\n",
    "              f'Validation Loss:\\t{val_loss}, '\n",
    "              f'Validation Accuracy:\\t{val_acc}, '\n",
    "              f'Validation Top 5 Accuracy:\\t{val_k_acc}\\n'\n",
    "              f'Test Loss:\\t{test_loss}, '\n",
    "              f'Test Accuracy:\\t{test_acc} '\n",
    "              f'Test Top 5 Accuracy:\\t{test_k_acc}'\n",
    "              f'\\n--------------------------------------------------------------------------------------------------------------------------\\n'\n",
    "            )\n",
    "\n",
    "    def main(self, compare_centralized = False, compression = False):\n",
    "        \n",
    "        self.compression = compression\n",
    "        if self.compression: \n",
    "            self.quantization_bits = 8 #default 8\n",
    "            self.quantization_thresh = 20000 # default 20000\n",
    "        \n",
    "        self.entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.sparseCategoricalAcc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.sparseTopKCategoricalAccuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=self.k)\n",
    "        self.client_optimizer =  tf.keras.optimizers.SGD(learning_rate= self.learning_rate, momentum=self.momentum, nesterov=self.nesterov)\n",
    "        self.server_optimzer = tf.keras.optimizers.SGD(learning_rate= 1.0)\n",
    "        \n",
    "        #read dataset\n",
    "        self.client_ids = None\n",
    "        self.scaler = StandardScaler()\n",
    "        utils = Utils()\n",
    "        reader = Reader(self.BASE_DIR + self.DATA_DIR, self.file, self.drop_index)\n",
    "        data = reader.get_data()\n",
    "\n",
    "        #clean from client_ids and label_ids and pass to scaler\n",
    "        if (\"IID.csv\" in self.file): \n",
    "            data = utils.create_clients(data, self.CLIENTS, strict = False)\n",
    "            reader.set_features(reader.get_features() + 1)\n",
    "            self.client_ids =  [i for i in range(0, self.CLIENTS)]\n",
    "\n",
    "        cols = [i for i in range(0, reader.get_features())]\n",
    "        del cols[0]\n",
    "        if \"app\" in self.file.lower():\n",
    "            del cols[2]\n",
    "        elif \"infected\" in self.file.lower():\n",
    "            del cols[8]\n",
    "        elif \"cosphere\" in self.file.lower():\n",
    "            del cols[3]\n",
    "\n",
    "        features = reader.get_features()\n",
    "        self.scaler.fit(data[:, cols])\n",
    "        \n",
    "        \n",
    "#         #TODO start loop für verschiedene clients\n",
    "        for clients in [2, 3]:\n",
    "            self.CLIENTS = clients\n",
    "            print(f\"\\n {clients} CLIENTS\\n\")\n",
    "\n",
    "            if ((self.file == \"App_usage_trace.txt\") or (self.file == \"top_90_apps.csv\") or (\"infected\" in self.file) or (\"Cosphere\" in self.file)): \n",
    "                # for subsets: renumber the client ids\n",
    "                data  = utils.map_ids(data.copy())\n",
    "                #create list of client ids\n",
    "                num_of_users = int((np.amax(data[:, 0]) + 1))\n",
    "                self.client_ids = list(range(0, num_of_users))\n",
    "                random.shuffle(self.client_ids)\n",
    "                self.client_ids = self.client_ids[:self.CLIENTS]\n",
    "                self.client_ids = sorted(self.client_ids)\n",
    "                \n",
    "            if (\"IID_2\" in self.file): \n",
    "                self.client_ids =  [i for i in range(0, self.CLIENTS)]\n",
    "\n",
    "            #federated training: create dataset per client\n",
    "            self.train_data = []\n",
    "            self.test_data = []\n",
    "\n",
    "            for id in self.client_ids:\n",
    "                \n",
    "                indicees = data[:, 0] == id\n",
    "                print(len(indicees))\n",
    "                print(data[:5])\n",
    "                former_shape = data[indicees, self.start_id:features].shape\n",
    "\n",
    "                #delete index 0 and 3 or 9, containing the label and the user id\n",
    "                client_x = np.delete( data[indicees, self.start_id:features], self.label_id-1, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
    "                #scale \n",
    "                client_x = self.scaler.transform(client_x)\n",
    "                client_y = data[indicees, self.label_id].reshape(-1, 1)\n",
    "\n",
    "                if len(client_x) > 1:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(client_x, client_y, test_size=0.2, random_state=42)    \n",
    "                    ds_train = self.create_dataset(X_train, y_train, self.use_tff)\n",
    "                    ds_test = self.create_dataset(X_test, y_test, self.use_tff)\n",
    "                    print(\"Client {}: Created  dataset\".format(id))\n",
    "\n",
    "                    self.train_data.append(ds_train)\n",
    "                    self.test_data.append(ds_test)\n",
    "                else:\n",
    "                    print(\"Could not generate datasets for client {} as there is just one entry in X_train\".format(id))\n",
    "                    self.client_ids.remove(id)\n",
    "\n",
    "            # Check format for TFF: needs to be in shape(None, dim)\n",
    "            #like eg:\n",
    "            # (TensorSpec(shape=(None, 3), dtype=tf.float64, name=None),\n",
    "            #  TensorSpec(shape=(None, 1), dtype=tf.float64, name=None)\n",
    "\n",
    "            #Check format for unfederated Learning: shape(batchsize, dim)\n",
    "            print(self.train_data[0].element_spec)\n",
    "            print(self.test_data[0].element_spec)\n",
    "\n",
    "            self.tff_utils = TFF_Utils()\n",
    "\n",
    "\n",
    "            #run without batch normalization\n",
    "            self.init_mlflow()\n",
    "            self.run_federated()\n",
    "\n",
    "            #run with batch normalization\n",
    "            self.use_bn = True\n",
    "            self.init_mlflow()\n",
    "            self.run_federated()\n",
    "\n",
    "            if compare_centralized:\n",
    "\n",
    "                self.use_tff = False\n",
    "                self.use_bn = False\n",
    "                self.init_mlflow()\n",
    "\n",
    "                #get same client ids, as with tff\n",
    "                mask = np.isin(data[:, 0], self.client_ids)\n",
    "                x = data[mask].copy() \n",
    "                #create dataset for centralized training\n",
    "                unfederated_train, unfederated_test, unfederated_val = self.create_unfederated_dataset(x, reader.get_features())\n",
    "                self.run_unfederated(unfederated_train, unfederated_test, unfederated_val,  (reader.get_features()-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_options = {    \"0\" : (\"App_usage_trace.txt\", 3996),\n",
    "                     \"1\" : (\"top_90_apps.csv\" , 3996), #171\n",
    "                     \"2\" : (\"10_infected.csv\", 3),\n",
    "                     \"3\" : (\"num_classes_infected_shuffled.csv\", 3),\n",
    "                     \"4\" : (\"Cosphere.csv\", 9972),\n",
    "                     \"5\" : (\"Cosphere_cropped.csv\", 9972),\n",
    "                     \"6\" : (\"top_90_apps_IID_2.csv\", 3996)\n",
    "                }\n",
    "\n",
    "used_option = \"2\"\n",
    "c_vs_fed_trainer = Centralized_vs_Federated_Trainer(file_name = input_options[used_option][0], \n",
    "                                                    number_of_classes =input_options[used_option][1] )\n",
    "c_vs_fed_trainer.main(compare_centralized=False, compression = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FL_vs_Non_FL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
