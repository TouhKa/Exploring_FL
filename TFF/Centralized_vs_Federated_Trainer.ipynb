{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM4kVhzbR_UK"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow-federated\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbo1O-krQDQh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, callbacks\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow_model_optimization.python.core.internal import tensor_encoding as te\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import random\n",
    "import time\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import mlflow \n",
    "\n",
    "from Reader import Reader\n",
    "from model.FLModel import FLModel\n",
    "from model.BNModel import BNModel\n",
    "from Utils import Utils\n",
    "from TFF_Utils import TFF_Utils\n",
    "from DefaultDenseQuantizeConfig import DefaultDenseQuantizeConfig\n",
    "\n",
    "tf.enable_eager_execution() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centralized_vs_Federated_Trainer:\n",
    "    \"\"\"\n",
    "    A class that compares centralized training with TensorFlow Federated's FedAVG as well as its own implementation of FedBN.\n",
    "    Additionally, lossy compression is compared in all federated settings.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        name of data file\n",
    "    number_of_classes : int\n",
    "        max value of class +label\n",
    "    E : int\n",
    "        number of local rounds\n",
    "    compression : bool \n",
    "        flag for compression\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    file : string\n",
    "        name of the input file containing the data\n",
    "    BASE_DIR : str\n",
    "        name of the directory containing all needed subdirectories for in- and output\n",
    "    DATA_DIR : str\n",
    "        name of the directory containing the input data\n",
    "    NUM_CLASSES : int\n",
    "        number of unique labels in the input file\n",
    "    client_ids : list\n",
    "        random int ids of the input datafile or passed input ids\n",
    "    CLIENTS : int\n",
    "        number of clients in total. when client_ids is passed this is set to len(client_ids)\n",
    "    EPOCHS : int\n",
    "    E : int \n",
    "        number of training epochs on the clients bevor a global update\n",
    "    k : int\n",
    "        specifies sparse top K aategorical accuracy\n",
    "    drop_index : bool\n",
    "        indicates if to delete the index column in the input cile\n",
    "    start_id : int\n",
    "        number of column where features starts\n",
    "    label_id :\n",
    "        column number of label\n",
    "    use_bn : bool\n",
    "        indicates if the FedBN should be used for training\n",
    "    use_tff : bool\n",
    "        indicates if zentralied or federated learing should be used\n",
    "    compression : bool\n",
    "        indicates if quantization should be used. only for federated learning\n",
    "    \n",
    "    BATCH_SIZE : int\n",
    "    PREFETCH_BUFFER : int\n",
    "    SHUFFLE_BUFFER : int\n",
    "    learning_rate : float\n",
    "    momentum : float\n",
    "    nesterov : bool\n",
    "    entropy_loss : tf.keras.losses.CategoricalCrossentropy\n",
    "    sparseCategoricalAcc : tf.keras.metrics.SparseCategoricalAccuracy \n",
    "    sparseTopKCategoricalAccuracy : sparseTopKCategoricalAccuracy\n",
    "    client_optimizer : tf.keras.optimizers.SGD\n",
    "    server_optimizer : tf.keras.optimizers.SGD\n",
    "    scaler : sklearn.preprocessing.StandardScaler\n",
    "    train_data : tf.data.Dataset \n",
    "    test_data : tf.data.Dataset \n",
    "    \n",
    "    Methods\n",
    "    ----------\n",
    "    get_summary_in_bytes( model)\n",
    "    init_mlflow():\n",
    "    split_input_target(input, target)\n",
    "    get_split(x, y)\n",
    "    create_dataset(x, y, use_tff = True)\n",
    "    create_unfederated_dataset\n",
    "    get_not_bn_idx\n",
    "    get_bn_idx\n",
    "    get_weights_by_idx\n",
    "    create_keras_model\n",
    "    create_keras_bn_model\n",
    "    quantize(layer)\n",
    "    dequantize(layer, min_range, max_range)\n",
    "    map_quantization(layer)\n",
    "    map_dequantization(weights, ranges, quant_idx)\n",
    "    get_min(layer)\n",
    "    get_max(layer)\n",
    "    model_fn\n",
    "    map_weights_ids\n",
    "    broadcast_encoder_fn\n",
    "    mean_encoder_fn\n",
    "    \n",
    "    run_federated()\n",
    "    \n",
    "        Submethods\n",
    "        ----------\n",
    "        server_init()\n",
    "        server_update(model, mean_client_weights)\n",
    "        server_update_fn(mean_client_weights)\n",
    "        initialize_fn()\n",
    "        next_fn(server_weights, federated_dataset)\n",
    "        evaluate(server_state, train_data, test_data, trainable_ids)\n",
    "        \n",
    "    run_unfederated(ds_train, ds_test, ds_val, input_dim)  \n",
    "    main(compare_centralized = False)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_name:str, number_of_classes:int, E:int, compression:bool = False):\n",
    "        self.file = file_name\n",
    "        self.NUM_CLASSES = number_of_classes\n",
    "        self.BASE_DIR = \"../\"\n",
    "        self.DATA_DIR = \"data/\"\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.PREFETCH_BUFFER = 10\n",
    "        self.SHUFFLE_BUFFER = 64\n",
    "        self.CLIENTS = 10\n",
    "        self.EPOCHS = 10\n",
    "        self.E = E # amount of local rounds       \n",
    "        self.drop_index = True\n",
    "        self.client_ids = None\n",
    "        \n",
    "        \n",
    "        if \"infected\" in self.file:\n",
    "            self.drop_index = False\n",
    "            self.start_id = 1\n",
    "            self.label_id = 9\n",
    "        elif (\"Cosphere\" in self.file):\n",
    "            self.start_id = 1\n",
    "            self.label_id = 4\n",
    "        else:\n",
    "            self.start_id = 1\n",
    "            self.label_id = 3\n",
    "\n",
    "        self.k = 10\n",
    "        self.use_bn = True\n",
    "        self.use_tff = True\n",
    "        self.learning_rate = 1e-1\n",
    "        self.momentum = 0.9\n",
    "        self.nesterov = False\n",
    "        \n",
    "        self.compression = compression\n",
    "        if self.compression: \n",
    "            self.quantization_bits = 8 #default 8\n",
    "            self.quantization_thresh = 10000\n",
    "\n",
    "        self.mode = 'MIN_FIRST'    \n",
    "        \n",
    "    def get_summary_in_bytes(self, model):\n",
    "        \"\"\"helper  function that outputs the summed size of all model layers in bits\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : keras.Model\n",
    "        \"\"\"\n",
    "        for layer in model.layers:\n",
    "            layer_size_in_bit = 0\n",
    "            for weight in layer.weights:\n",
    "                size = 0\n",
    "                if len(weight.shape) > 1:\n",
    "                    size = weight.shape[0] * weight.shape[1]\n",
    "                    \n",
    "                if len(weight.shape) == 1:\n",
    "                    size = weight.shape[0]\n",
    "                \n",
    "                if weight.dtype == 'float32':\n",
    "                    layer_size_in_bit += (size * 32)\n",
    "                    \n",
    "                elif weight.dtype == 'int8':\n",
    "                    layer_size_in_bit += (size * 8)\n",
    "                else:\n",
    "                    print(f\"unsupported datatype: {weight.dtype}\")\n",
    "                    \n",
    "            print(layer, layer_size_in_bit)        \n",
    "            \n",
    "    #mlflow setup\n",
    "    def init_mlflow(self): \n",
    "        \"\"\"sets up an mlflow run\"\"\"\n",
    "        mlflow.set_experiment(self.file)\n",
    "        mlflow.end_run()\n",
    "        mlflow.start_run()\n",
    "        mlflow_experiment_id = mlflow.get_experiment_by_name(self.file).experiment_id\n",
    "        mlflow_run_id = mlflow.active_run().info.run_id\n",
    "        log_path = \"mlruns/\" + str(mlflow_experiment_id) + \"/\" + str(mlflow_run_id) + \"/\" + \"artifacts\" + \"/\"\n",
    "        \n",
    "        mlflow.log_param(\"run_id\", mlflow_run_id)\n",
    "        mlflow.log_param(\"local_rounds\", self.k)\n",
    "        mlflow.log_param(\"use_bn\", self.use_bn)\n",
    "        mlflow.log_param(\"use_tff\", self.use_tff)\n",
    "        if self.compression: \n",
    "            mlflow.log_param(\"use_compression\", self.compression)\n",
    "            mlflow.log_param(\"quant_bits\", self.quantization_bits)\n",
    "            mlflow.log_param(\"threshold\", self.quantization_thresh)\n",
    "            mlflow.set_tags({\"day\":\"15.09.2020\"})\n",
    "            mlflow.log_param(\"mode\", self.mode)\n",
    "\n",
    "        mlflow.log_param(\"lr\", self.learning_rate)\n",
    "        mlflow.log_param(\"momentum\", self.momentum)\n",
    "        mlflow.log_param(\"nesterov\", self.nesterov)\n",
    "        mlflow.log_param(\"batch_size\", self.BATCH_SIZE)\n",
    "        mlflow.log_param(\"clients\", self.CLIENTS)\n",
    "        mlflow.log_param(\"epochs\", self.EPOCHS)\n",
    "        mlflow.log_param(\"classes\", self.NUM_CLASSES)\n",
    "            \n",
    "    def split_input_target(self, input, target):\n",
    "        \"\"\"helper function for formatting the TFF dataset in the needed format\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Tensor\n",
    "            features\n",
    "        target : Tensor\n",
    "            label\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "        tensor\n",
    "        \"\"\"\n",
    "        return input, target\n",
    "\n",
    "    def create_dataset(self, x, y):\n",
    "        \"\"\"creates datasets in a specific format depending if zentralized or federated learning is used\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array()\n",
    "        y : numpy.array()\n",
    "        \n",
    "        Returns\n",
    "        ---- \n",
    "        tf.data.Dataset\n",
    "        \"\"\"\n",
    "        ds =  tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "        if self.use_tff:\n",
    "            return (\n",
    "            ds.repeat(self.EPOCHS).shuffle(self.SHUFFLE_BUFFER)\n",
    "            .map(self.split_input_target)).batch(self.BATCH_SIZE) \n",
    "        else:\n",
    "            return ds.repeat(self.BATCH_SIZE * self.EPOCHS).shuffle(self.SHUFFLE_BUFFER).batch(self.BATCH_SIZE,drop_remainder = True) \n",
    "\n",
    "    def get_split(self, x, y):\n",
    "        \"\"\"Split arrays or matrices into random train and test subsets\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array()\n",
    "        y : numpy.array()\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        List \n",
    "            containing train-test split of inputs.\n",
    "        \"\"\"\n",
    "        return train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    def create_unfederated_dataset(self, x, features):\n",
    "        \"\"\" creates an scaled, zentralized train- and test-dataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy.array()\n",
    "            containing the whole dataset\n",
    "        features : int\n",
    "            number of input features\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tf.data.Dataset\n",
    "            train dataset\n",
    "        tf.data.Dataset\n",
    "            test dataset\n",
    "        tf.data.Dataset\n",
    "            validation dataset\n",
    "        \"\"\"\n",
    "        former_shape = x[:, self.start_id:features].shape\n",
    "        client_x = np.delete( x[:, self.start_id:features], self.label_id-1, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
    "        client_x = self.scaler.transform(client_x)\n",
    "        client_y = x[:, self.label_id].reshape(-1, 1)\n",
    "        X_train, X_test, y_train, y_test = self.get_split(client_x, client_y)\n",
    "        X_train, X_val, y_train, y_val = self.get_split(X_train, y_train)\n",
    "        train_data = self.create_dataset(X_train, y_train)\n",
    "        test_data = self.create_dataset(X_test, y_test)\n",
    "        val_data = self.create_dataset(X_val, y_val)\n",
    "        return train_data, test_data, val_data\n",
    "    \n",
    "    def get_not_bn_idx(self, trainable_variables):\n",
    "        \"\"\"\n",
    "        returns the indicees of a list of trainable weights which are not batch normalization layers\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        trainable_variables : list \n",
    "            list of weight tensors\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        list \n",
    "            list of indicees\n",
    "        \"\"\"\n",
    "        \n",
    "        new_trainable_weights = []\n",
    "        train_var_idx = []\n",
    "        for idx, bn_weights in enumerate(trainable_variables):\n",
    "            if 'batch_normalization' not in bn_weights.name:\n",
    "                train_var_idx.append(idx)\n",
    "        return train_var_idx\n",
    "\n",
    "    def get_bn_idx(self, trainable_variables):\n",
    "        \"\"\" returns the indicees of a list of trainable weights which are batch normalization layers\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        trainable_variables: list\n",
    "            list of weight tensors\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list of indicees\n",
    "        \"\"\"\n",
    "        \n",
    "        new_trainable_weights = []\n",
    "        train_var_idx = []\n",
    "        for idx, bn_weights in enumerate(trainable_variables):\n",
    "            if 'batch_normalization' in bn_weights.name:\n",
    "                train_var_idx.append(idx)\n",
    "        return train_var_idx\n",
    "\n",
    "    def get_weights_by_idx(self, trainable_variables, var_ids):\n",
    "        \"\"\" returns a new list of selected weight tensors\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        trainable_variables : list\n",
    "            list of weight tensors\n",
    "        var_ids : list\n",
    "            list of to be selected weight-ids\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        list \n",
    "            list of weight tensors\n",
    "        \"\"\"\n",
    "        new_weights = []  \n",
    "        for i in var_ids:\n",
    "            new_weights.append(trainable_variables[i])\n",
    "        return new_weights\n",
    "\n",
    "    def create_keras_model(self, input_dim):\n",
    "        \"\"\" returns an instance of a keras model for FedAVG\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            number of input connections\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        keras.model\n",
    "        \"\"\"\n",
    "        return tf.keras.models.Sequential([\n",
    "          tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "          tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "          tf.keras.layers.Dense(self.NUM_CLASSES, activation='softmax'),\n",
    "        ])\n",
    "\n",
    "    def create_keras_bn_model(self, input_dim):\n",
    "        \"\"\" returns an instance of a keras model for FedBN\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            number of input connections\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        keras.model\n",
    "        \"\"\"\n",
    "        return tf.keras.models.Sequential([\n",
    "          tf.keras.layers.BatchNormalization(input_shape=(input_dim,)),\n",
    "          tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "          tf.keras.layers.BatchNormalization(),\n",
    "          tf.keras.layers.Dense(self.NUM_CLASSES, activation='softmax'),\n",
    "        ])\n",
    "        \n",
    "    def quantize(self, layer):\n",
    "        \"\"\" performs quantization on a 32-bit float weight tensor. Returns an 8-bit-integer-tensor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer: tensor\n",
    "            weight tensor to be quantized\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "        \"\"\"\n",
    "        min_range = self.get_min(layer)\n",
    "        max_range = self.get_max(layer)\n",
    "        return tf.quantization.quantize(layer, min_range, max_range, tf.qint32, mode = self.mode)\n",
    "    \n",
    "    def dequantize(self, layer, min_range, max_range):\n",
    "        \"\"\" performs dequantization on a 8-bit int weight tensor. Returns an 32-bit-float-tensor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer: tensor\n",
    "            weight tensor to be dequantized      \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "        \"\"\"\n",
    "        return tf.quantization.dequantize(layer, min_range, max_range, mode=self.mode, dtype=tf.dtypes.float32)\n",
    "    \n",
    "    def map_quantization(self, weights):\n",
    "        \"\"\" Performs quantization, if the size of an weigth tensor is bigger than a threshold \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : list\n",
    "            list of weight tensors     \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        list \n",
    "            of all weights including the quantized ones\n",
    "        list \n",
    "            list of tupel containing min and max range for dequantization\n",
    "        list\n",
    "            list of indicees which weights to dequantize\n",
    "        \n",
    "        \"\"\"\n",
    "        quantized_weights = []\n",
    "        ranges = []\n",
    "        quant_idx = []\n",
    "        for idx, weight in enumerate(weights):\n",
    "            if len(weight.shape) > 1:\n",
    "                size = weight.shape[0] * weight.shape[1]\n",
    "            elif len(weight.shape) == 1:\n",
    "                size = weight.shape[0]\n",
    "            if size > self.quantization_thresh:\n",
    "                quantized_weight, mi, ma = self.quantize(weight)\n",
    "                quantized_weights.append(quantized_weight)\n",
    "                ranges.append((mi, ma))\n",
    "                quant_idx.append(idx)\n",
    "            else: \n",
    "                quantized_weights.append(weight)\n",
    "        print(f\"\\nquanization: \\n\\n{quantized_weights, quant_idx}\\n\\n----\")\n",
    "        return quantized_weights, ranges, quant_idx\n",
    "    \n",
    "    def map_dequantization(self, weights, ranges, quant_idx):\n",
    "        \"\"\" Perfoms dequantization \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : list\n",
    "            list of all weight tensors\n",
    "        ranges : list \n",
    "            list of tupel with min-range and max-range\n",
    "        quant_idx : list\n",
    "            list of indices of weights to be dequantified\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "            list of float32-tensors\n",
    "        \"\"\"\n",
    "        dequantized_weights = []\n",
    "        for idx, weight in enumerate(weights):\n",
    "            if idx in quant_idx:\n",
    "                range = ranges[0]\n",
    "                ranges.pop(0)\n",
    "                dequantized_weight = self.dequantize(weight, range[0], range[1])\n",
    "                dequantized_weights.append(dequantized_weight)\n",
    "            else: \n",
    "                dequantized_weights.append(weight)\n",
    "        \n",
    "        return dequantized_weights\n",
    "    \n",
    "    def get_min(self, layer):\n",
    "        \"\"\" Returns the smallest value of a tensor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer: tensor\n",
    "        \"\"\"\n",
    "        return tf.math.reduce_min(layer, axis=None)\n",
    "    \n",
    "    def get_max(self, layer):\n",
    "        \"\"\" Returns the biggest value of a tensor\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        layer: tensor\n",
    "        \"\"\"\n",
    "        return tf.math.reduce_max(layer, axis=None)\n",
    "        \n",
    " \n",
    "    def model_fn(self):\n",
    "        \"\"\" Create instance of a keras model depending on which federated strategie (FedAVG, FedBN) to use \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        keras.model\n",
    "        \"\"\"\n",
    "        \n",
    "      # Note: We must create a new model here, and not capture it from an external scope. \n",
    "      # TFF will call this within different graph contexts.\n",
    "\n",
    "        if self.use_bn:\n",
    "          keras_model = self.create_keras_bn_model(self.test_data[0].element_spec[0].shape[1])\n",
    "\n",
    "        else:\n",
    "          keras_model = self.create_keras_model(self.test_data[0].element_spec[0].shape[1])\n",
    " \n",
    "        return tff.learning.from_keras_model(\n",
    "          keras_model,\n",
    "          input_spec = self.train_data[0].element_spec,\n",
    "          loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "          metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "                   tf.keras.metrics.SparseTopKCategoricalAccuracy(k=self.k)]) \n",
    "\n",
    "\n",
    "    def map_weights_ids(self):\n",
    "        \"\"\" Filter the weights for FedBN which are not from batch normalization layers\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "        list of weight tensors\n",
    "        \"\"\"\n",
    "        model = self.create_keras_bn_model(self.train_data[0].element_spec[0].shape[1])\n",
    "        trainable_variables = model.trainable_variables\n",
    "        all_weights_name = []\n",
    "        train_weights_ids = []\n",
    "\n",
    "        for layer in model.layers:\n",
    "            for weight in layer.weights:\n",
    "                all_weights_name.append(weight.name)\n",
    "\n",
    "        for var in trainable_variables:\n",
    "            for idx, name in enumerate(all_weights_name):\n",
    "                if var.name == name:\n",
    "                    if 'batch_normalization' not in name:\n",
    "                        train_weights_ids.append(idx)\n",
    "\n",
    "        return train_weights_ids\n",
    "\n",
    "    def broadcast_encoder_fn(self, value):\n",
    "        \"\"\"Broadcasting step: Function for building encoded broadcast.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : tensor\n",
    "          tensor to be quantized if bigger than a threshold\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          if tensor bigger than threshold: returns instance of quantization encoder\n",
    "          else: returns instance of default encoder without any specific compression\n",
    "\n",
    "        \"\"\"\n",
    "        spec = tf.TensorSpec(value.shape, value.dtype)\n",
    "        if value.shape.num_elements() > self.quantization_thresh:\n",
    "            return te.encoders.as_simple_encoder(\n",
    "                te.encoders.uniform_quantization(bits=self.quantization_bits), spec)\n",
    "        else:\n",
    "            return te.encoders.as_simple_encoder(te.encoders.identity(), spec)\n",
    "\n",
    "\n",
    "    def mean_encoder_fn(self, tensor_spec):\n",
    "        \"\"\"Aggregation step: Function for building a GatherEncoder\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value : tensor\n",
    "          tensor to be quantized if bigger than a threshold\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          if tensor bigger than threshold: returns instance of quantization encoder\n",
    "          else: returns instance of default encoder without any specific compression\n",
    "\n",
    "        \"\"\"\n",
    "        spec = tf.TensorSpec(tensor_spec.shape, tensor_spec.dtype)\n",
    "        if tensor_spec.shape.num_elements() > self.quantization_thresh:\n",
    "            return te.encoders.as_gather_encoder(\n",
    "                te.encoders.uniform_quantization(bits= self.quantization_bits), spec)\n",
    "        else:\n",
    "            return te.encoders.as_gather_encoder(te.encoders.identity(), spec)\n",
    "\n",
    "    def run_federated(self):\n",
    "        \"\"\" Handles the federated learning. \n",
    "        \n",
    "        First, this contains some inner classes for dealing with FedBN. \n",
    "        Placement outside throws errors regarding scope. \n",
    "        \n",
    "        Second, this runs the training and evaluation process of FedAVG or FedBN, depending of the value of'self.use_bn'\n",
    "        \n",
    "        \"\"\"        \n",
    "        broadcast_process = None\n",
    "        mean_factory = None\n",
    "        if self.compression and (not self.use_bn):  \n",
    "            mean_factory = tff.aggregators.MeanFactory(\n",
    "                value_sum_factory = tff.aggregators.EncodedSumFactory(self.mean_encoder_fn),\n",
    "                weight_sum_factory = tff.aggregators.EncodedSumFactory(self.mean_encoder_fn))   \n",
    "            \n",
    "            broadcast_process = tff.learning.framework.build_encoded_broadcast_process_from_model(\n",
    "                                         self.model_fn, self.broadcast_encoder_fn)\n",
    "            \n",
    "\n",
    "        # ---custom fed avg implementation for batch normalization-----\n",
    "        # --------------------------START------------------------------\n",
    "        @tff.tf_computation\n",
    "        def server_init():\n",
    "            \"\"\"FedBN: creates an instance of the specified keras model and filters the non-batch-normalization-weights\n",
    "     \n",
    "            Returns\n",
    "            -------\n",
    "            list of all weights which are trainable and not from a batch norralization layer\n",
    "            \"\"\"\n",
    "            \n",
    "            model = self.model_fn()\n",
    "            trainable_variables = model.trainable_variables\n",
    "            non_bn_ids = self.get_not_bn_idx(trainable_variables)\n",
    "            return self.get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "        \n",
    "        whimsy_model = self.model_fn()\n",
    "        tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)\n",
    "        model_weights_type = server_init.type_signature.result\n",
    "        federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
    "        federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
    "\n",
    "        @tf.function\n",
    "        def server_update(model, mean_client_weights):\n",
    "            \"\"\"FedBN: Updates the server weights with the average of the client model weights.\n",
    "            \n",
    "            if 'self.use_compression' is True: Quantizes and dequantizes the post-training-weights. \n",
    "            This way the information loss can be later analyzed\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            model : keras.model\n",
    "            mean_client_weights : federated value with placement of the averaged client weights\n",
    "                list of tensors\n",
    "                \n",
    "            Returns\n",
    "            ------\n",
    "            list of all weights which are trainable and not from a batch normralization layer\n",
    "            \"\"\"\n",
    "            \n",
    "            updated_model_weights = []\n",
    "            trainable_variables = model.trainable_variables\n",
    "\n",
    "            non_bn_ids = self.get_not_bn_idx(trainable_variables)\n",
    "            bn_ids = self.get_bn_idx(trainable_variables)\n",
    "            non_bn_weights = self.get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "            bn_weights = self.get_weights_by_idx(trainable_variables, bn_ids)\n",
    "\n",
    "            tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                                    non_bn_weights, mean_client_weights)\n",
    "            for i in range(len(trainable_variables)):\n",
    "                if i in non_bn_ids:\n",
    "                    j = non_bn_ids.index(i)\n",
    "                    updated_model_weights.append(non_bn_weights[j])\n",
    "                else: \n",
    "                    j = bn_ids.index(i)\n",
    "                    updated_model_weights.append(bn_weights[j])\n",
    "                    \n",
    "            #--------- quantize and dequantize---------\n",
    "            if self.use_bn:\n",
    "                quantized_weights, min_max, range_idx = self.map_quantization(non_bn_weights)\n",
    "                return self.map_dequantization(quantized_weights, min_max, range_idx)\n",
    "            #---------------------------\n",
    "            \n",
    "            return non_bn_weights\n",
    "\n",
    "\n",
    "        @tf.function\n",
    "        def client_update(model, dataset, server_weights, client_optimizer):\n",
    "            \"\"\"FedBN: Updates the clients weights with the new global server weights.\n",
    "            \n",
    "            if 'self.use_compression' is True: Quantizes and dequantizes the post-training-weights. \n",
    "            This way the information loss can be later analyzed\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            dataset: tf.data.dataset\n",
    "                lokal client dataset\n",
    "            server_weights : tensor\n",
    "            client_optimizer: tf.keras.optimizers.SGD\n",
    "                instance of the corresponding client optimizer\n",
    "                \n",
    "            Returns\n",
    "            ------\n",
    "            list of all weights which are trainable and not from a batch normalization layer\n",
    "            \"\"\"\n",
    "            \n",
    "            updated_clients_weights = []\n",
    "            trainable_variables = model.trainable_variables\n",
    "            \n",
    "            #get ids of non_batch normalization weights\n",
    "            non_bn_ids = self.get_not_bn_idx(trainable_variables)\n",
    "            mlflow.log_param(\"number of updatable non-BN weights\", len(non_bn_ids))\n",
    "            \n",
    "            #get ids of batch normalization weights\n",
    "            bn_ids = self.get_bn_idx(trainable_variables)\n",
    "            \n",
    "            non_bn_weights = self.get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "            bn_weights = self.get_weights_by_idx(trainable_variables, bn_ids)\n",
    "            \n",
    "            # Assign the mean client weights to the server model.\n",
    "            tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                                    non_bn_weights, server_weights)\n",
    "\n",
    "            #update any weight which is not from a bn layer\n",
    "            for i in range(len(trainable_variables)):\n",
    "                if i in non_bn_ids:\n",
    "                    j = non_bn_ids.index(i)\n",
    "                    updated_clients_weights.append(non_bn_weights[j])\n",
    "                    \n",
    "                    #log shape of weight \n",
    "                    #mlflow.log_param(f\"{j} Shape of updatable non-BN weight\", non_bn_weights[j].shape)\n",
    "                else: \n",
    "                    j = bn_ids.index(i)\n",
    "                    updated_clients_weights.append(bn_weights[j])\n",
    "\n",
    "            client_weights = updated_clients_weights\n",
    "\n",
    "            for epoch in range(self.E):\n",
    "                for batch in dataset:\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        outputs = model.forward_pass(batch)\n",
    "                    grads = tape.gradient(outputs.loss, client_weights)\n",
    "                    grads_and_vars = zip(grads, client_weights)\n",
    "                    client_optimizer.apply_gradients(grads_and_vars)\n",
    "                    \n",
    "            #--------- quantize and dequantize---------\n",
    "            if self.use_bn:\n",
    "                quantized_weights, min_max, range_idx = self.map_quantization(non_bn_weights)\n",
    "                return self.map_dequantization(quantized_weights, min_max, range_idx)\n",
    "            #---------------------------\n",
    "\n",
    "            return non_bn_weights\n",
    "\n",
    "        @tff.tf_computation(tf_dataset_type, model_weights_type)\n",
    "        def client_update_fn(tf_dataset, server_weights):\n",
    "            \"\"\"FedBN:Instantiates the necessary objects for a client step\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            tf_dataset : tf.data.dataset\n",
    "            server_weights : federated_map\n",
    "                Placement of the corresponding TF values\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            list of all weights which are trainable and not from a batch normalization layer\n",
    "            \"\"\"\n",
    "            model = self.model_fn()\n",
    "            client_optimizer = tf.keras.optimizers.SGD(learning_rate= self.learning_rate, momentum=self.momentum, nesterov=self.nesterov)\n",
    "            return client_update(model, tf_dataset, server_weights, client_optimizer)\n",
    "\n",
    "        @tff.tf_computation(model_weights_type)\n",
    "        def server_update_fn(mean_client_weights):\n",
    "            \"\"\"FedBN:Instantiates the necessary objects for assigning the new server weights\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            mean_client_weights : federated_map\n",
    "                Placement of the corresponding TF values\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            list of all weights which are trainable and not from a batch normalization layer\n",
    "            \"\"\"\n",
    "            model = self.model_fn()\n",
    "            return server_update(model, mean_client_weights)\n",
    "\n",
    "        @tff.federated_computation\n",
    "        def initialize_fn():\n",
    "            \"\"\"FedBN: Start the federated learning by braodcasting the initial server weights to all clients\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            Federated Value with placement of the server weights\n",
    "            \"\"\"\n",
    "            return tff.federated_value(server_init(), tff.SERVER)\n",
    "\n",
    "\n",
    "        @tff.federated_computation(federated_server_type, federated_dataset_type)\n",
    "        def next_fn(server_weights, federated_dataset):\n",
    "            \"\"\"FedBN: Handels the lifecycle of the federated learning\n",
    "            \n",
    "            First the client weights are broadcasted to the clients.\n",
    "            The clients train their local model with 'self.E' steps\n",
    "            Aferwards the client weights are aggregated and averaged.\n",
    "            Finally, the new server weights are updated.\n",
    "            \n",
    "            Note: All variables of federated operations are TFF Variables and not directly accessible. \n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            server_weights: list\n",
    "                 list if server weights passed by 'server_update'\n",
    "            federated_dataset\n",
    "                list: of datasets\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            call to 'server_update_fn'\n",
    "            \"\"\"\n",
    "\n",
    "           # Broadcast the server weights to the clients.\n",
    "            server_weights_at_client = tff.federated_broadcast(server_weights)\n",
    "\n",
    "            # Each client computes their updated weights.\n",
    "            client_weights = tff.federated_map(\n",
    "                client_update_fn, (federated_dataset, server_weights_at_client))\n",
    "\n",
    "            # The server averages these updates.\n",
    "            mean_client_weights = tff.federated_mean(client_weights)\n",
    "\n",
    "            # # The server updates its model.\n",
    "            server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
    "                             \n",
    "            return server_weights\n",
    "\n",
    "        def evaluate(server_state, train_data, test_data, trainable_ids):\n",
    "            \"\"\"FedBN: This methods runs the global model in testmode on both the training set and the test set\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            server_state : tff.learning.framework.ServerState\n",
    "                 contains the currently global model\n",
    "                 \n",
    "            train_data: tf.data.dataset\n",
    "                for all clients\n",
    "            test_data: tf.data.dataset\n",
    "                for all clients\n",
    "            trainable_ids: list\n",
    "                list of weight indicees. Just the non-batch-normalization-weights are passed the the new model instance\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            Dictionary of the specified metrics for training and testing\n",
    "            \"\"\"\n",
    "            \n",
    "            server_weights = []\n",
    "            acc_mean, loss_mean, k_acc_mean = [], [], []\n",
    "            model = self.create_keras_bn_model(test_data[0].element_spec[0].shape[1])\n",
    "            model.compile(\n",
    "                    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "                            tf.keras.metrics.SparseTopKCategoricalAccuracy(k=self.k)]  \n",
    "            )\n",
    "            weights = model.get_weights()\n",
    "\n",
    "            for idx, weight_id in enumerate(trainable_ids):\n",
    "                weights[weight_id] = np.array(server_state[idx])\n",
    "\n",
    "            model.set_weights(weights)\n",
    "            print(\"\\t--Training--\\t\")\n",
    "\n",
    "            for batch in self.train_data:\n",
    "                loss, acc, k_acc = model.evaluate(batch, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "                loss_mean.append(loss)\n",
    "                acc_mean.append(acc)\n",
    "                k_acc_mean.append(k_acc)\n",
    "            train_loss = statistics.mean(loss_mean)\n",
    "            train_acc = statistics.mean(acc_mean)\n",
    "            train_k_acc = statistics.mean(k_acc_mean)\n",
    "            acc_mean, loss_mean, k_acc_mean = [], [], []\n",
    "\n",
    "            print(\"\\t--Testing--\\t\")\n",
    "            for batch in self.test_data:\n",
    "                loss, acc, top_k_acc = model.evaluate(batch, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "                loss_mean.append(loss)\n",
    "                acc_mean.append(acc)\n",
    "                k_acc_mean.append(k_acc)\n",
    "            test_loss = statistics.mean(loss_mean)\n",
    "            test_acc = statistics.mean(acc_mean)\n",
    "            test_k_acc = statistics.mean(k_acc_mean)\n",
    "            return {\"Train_acc\":train_acc, \n",
    "                    f\"Train_{self.k}_acc\":train_k_acc, \n",
    "                    \"Train_loss\":train_loss, \n",
    "                    \"Test_acc\":test_acc, \n",
    "                    f\"Test_{self.k}_acc\":test_k_acc,\n",
    "                    \"Test_loss\":test_loss,}\n",
    "        # -------------------END---------------------\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            #FedBN\n",
    "            if self.use_bn:    \n",
    "                trainable_ids = self.map_weights_ids()\n",
    "                fed_avg = tff.templates.IterativeProcess(\n",
    "                                        initialize_fn=initialize_fn,\n",
    "                                        next_fn=next_fn\n",
    "                                        )\n",
    "\n",
    "                state = fed_avg.initialize()\n",
    "                for round_num in range(self.EPOCHS):  \n",
    "                    state = fed_avg.next(state, self.train_data)\n",
    "                    \n",
    "                    metrics = evaluate(state, self.train_data, self.test_data, trainable_ids)\n",
    "                    for name, value in metrics.items():\n",
    "                        mlflow.log_metric(name, value, round_num)\n",
    "                        print(round_num, name, value)\n",
    "                        \n",
    "            #FedAVG\n",
    "            else:\n",
    "                fed_avg = tff.learning.build_federated_averaging_process(\n",
    "                    self.model_fn,\n",
    "                    client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= self.learning_rate, momentum=self.momentum, nesterov=self.nesterov), \n",
    "                    server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= 1.0),\n",
    "                    broadcast_process = broadcast_process,\n",
    "                    model_update_aggregation_factory = mean_factory\n",
    "                    )\n",
    "\n",
    "                state = fed_avg.initialize()\n",
    "                \n",
    "                #-----\n",
    "                #just usable for FedAVG\n",
    "                environment = self.tff_utils.set_sizing_environment()\n",
    "                #-----\n",
    "                \n",
    "                for round_num in range(self.EPOCHS):\n",
    "                    state, metrics = fed_avg.next(state, self.train_data)\n",
    "                    # Note: training metrics reported by the iterative training process \n",
    "                    #generally reflect the performance of the model at the beginning of the training round\n",
    "                    \n",
    "                    #---\n",
    "                    size_info = environment.get_size_info()\n",
    "                    broadcasted_bits = size_info.broadcast_bits[-1]\n",
    "                    aggregated_bits = size_info.aggregate_bits[-1]\n",
    "                    mlflow.log_metric('cumulative_broadcasted_bits', broadcasted_bits, round_num)\n",
    "                    mlflow.log_metric('cumulative_aggregated_bits', aggregated_bits, round_num)\n",
    "                    print(broadcasted_bits, aggregated_bits)\n",
    "                    \n",
    "                    print('round {:2d}, metrics={}, broadcasted_bits={}, aggregated_bits={}'\n",
    "                          .format(round_num, metrics, \n",
    "                                  self.tff_utils.format_size(broadcasted_bits), \n",
    "                                  self.tff_utils.format_size(aggregated_bits)\n",
    "                            )\n",
    "                         )\n",
    "                    #---\n",
    "                    \n",
    "                    for name, value in metrics['train'].items():\n",
    "                        print(round_num, name, value)\n",
    "\n",
    "                    evaluation = tff.learning.build_federated_evaluation(self.model_fn)  \n",
    "                    test_metrics = evaluation(state.model, self.test_data)\n",
    "                    for name, value in test_metrics.items():\n",
    "                        mlflow.log_metric(name, value, round_num)\n",
    "                        print(round_num, name, value)\n",
    "\n",
    "    def run_unfederated(self, ds_train, ds_test, ds_val, input_dim):\n",
    "        \"\"\"Handels centralized training with TensorFlow\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ds_train : tf.data.dataset\n",
    "        ds_test : tf.data.dataset\n",
    "        ds_val : tf.data.dataset\n",
    "        input_dim: int\n",
    "            number of input connections\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                    min_delta=0.01, \n",
    "                                    patience=2, \n",
    "                                    verbose=0, \n",
    "                                    mode='auto', \n",
    "                                    baseline=None, \n",
    "                                    restore_best_weights=True)\n",
    "        if self.use_bn:\n",
    "            model = BNModel(self.NUM_CLASSES)\n",
    "        else:\n",
    "            model = FLModel(self.NUM_CLASSES)\n",
    "        model.compile(\n",
    "                    optimizer= self.client_optimizer, \n",
    "                    loss= \"sparse_categorical_crossentropy\", \n",
    "                    metrics= [\n",
    "                              self.sparseCategoricalAcc, \n",
    "                              self.sparseTopKCategoricalAccuracy\n",
    "                              ]\n",
    "                    )\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            with tf.device('/gpu:0'):\n",
    "                history = model.fit(\n",
    "                                  ds_train,\n",
    "                                  steps_per_epoch=64, \n",
    "                                  validation_data = ds_val, \n",
    "                                  verbose=0,\n",
    "                                  callbacks = [early_stopping_callback])     \n",
    "\n",
    "            loss = round(history.history[\"loss\"][0], 8)\n",
    "            acc = round(history.history[\"sparse_categorical_accuracy\"][0], 8)\n",
    "            k_acc =  round(history.history[\"sparse_top_k_categorical_accuracy\"][0], 8)\n",
    "            val_loss =  round(history.history['val_loss'][0], 8)\n",
    "            val_acc = round(history.history['val_sparse_categorical_accuracy'][0], 8)\n",
    "            val_k_acc =  round(history.history['val_sparse_top_k_categorical_accuracy'][0], 8)\n",
    "                    \n",
    "            with tf.device('/gpu:0'):\n",
    "                    test_loss, test_acc, test_k_acc = model.evaluate(ds_test, batch_size=self.BATCH_SIZE, verbose=0)\n",
    "\n",
    "            mlflow.log_metric(\"Loss/train\", loss, epoch)\n",
    "            mlflow.log_metric(\"Acc/train\", acc, epoch)\n",
    "            mlflow.log_metric(\"K_acc/train\", k_acc, epoch)\n",
    "\n",
    "            mlflow.log_metric(\"Loss/validation\", val_loss, epoch)\n",
    "            mlflow.log_metric(\"Acc/validation\", val_acc, epoch)\n",
    "            mlflow.log_metric(\"K_acc/validation\", val_k_acc, epoch)\n",
    "\n",
    "            mlflow.log_metric(\"Loss/test\", test_loss, epoch)\n",
    "            mlflow.log_metric(\"Acc/test\", test_acc, epoch)\n",
    "            mlflow.log_metric(\"K_acc/test\", test_k_acc, epoch)\n",
    "\n",
    "            print(\n",
    "              f'Epoch: {epoch},\\n'\n",
    "              f'Train Loss:\\t{loss}, '\n",
    "              f'Train Accuracy:\\t{acc}, '\n",
    "              f'Train Top 5 Accuracy:\\t{k_acc}\\n'\n",
    "              f'Validation Loss:\\t{val_loss}, '\n",
    "              f'Validation Accuracy:\\t{val_acc}, '\n",
    "              f'Validation Top 5 Accuracy:\\t{val_k_acc}\\n'\n",
    "              f'Test Loss:\\t{test_loss}, '\n",
    "              f'Test Accuracy:\\t{test_acc} '\n",
    "              f'Test Top 5 Accuracy:\\t{test_k_acc}'\n",
    "              f'\\n--------------------------------------------------------------------------------------------------------------------------\\n'\n",
    "            )\n",
    "\n",
    "    def main(self, compare_centralized = False):\n",
    "        \"\"\"Handels all possible setups on the same client ids so comparission of the results is possible\n",
    "        \n",
    "        Reads the desired input file and creates #'self.CLIENTS' random clients\n",
    "        Runs loop on a range of clients\n",
    "            Creates datasets from the subset of client_ids\n",
    "            Setup:\n",
    "            Run centralized learning\n",
    "            Run FedAVG without compression\n",
    "            Run FedBN without compression\n",
    "            Loop on different compression thresholds:\n",
    "                Run FedAVG with compression an a specified threshold\n",
    "                Run FedBN with compression an a specified threshold\n",
    "                \n",
    "        Note:For each of theses runs there are logging files from mlflow\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        compare_centralized : bool\n",
    "            Indicates if centralized training should be part of the setup\n",
    "        \"\"\"\n",
    "               \n",
    "        self.entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.sparseCategoricalAcc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.sparseTopKCategoricalAccuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=self.k)\n",
    "        self.client_optimizer =  tf.keras.optimizers.SGD(learning_rate= self.learning_rate, momentum=self.momentum, nesterov=self.nesterov)\n",
    "        self.server_optimzer = tf.keras.optimizers.SGD(learning_rate= 1.0)\n",
    "        \n",
    "        #read dataset\n",
    "        self.scaler = StandardScaler()\n",
    "        utils = Utils()\n",
    "        reader = Reader(self.BASE_DIR + self.DATA_DIR, self.file, self.drop_index)\n",
    "        data = reader.get_data()\n",
    "\n",
    "        #clean from client_ids and label_ids and pass to scaler\n",
    "        if (\"IID.csv\" in self.file): \n",
    "            data = utils.create_clients(data, self.CLIENTS, strict = False)\n",
    "            reader.set_features(reader.get_features() + 1)\n",
    "            self.client_ids =  [i for i in range(0, self.CLIENTS)]\n",
    "\n",
    "        cols = [i for i in range(0, reader.get_features())]\n",
    "        del cols[0]\n",
    "        if \"app\" in self.file.lower():\n",
    "            del cols[2]\n",
    "        elif \"infected\" in self.file.lower():\n",
    "            del cols[8]\n",
    "        elif \"cosphere\" in self.file.lower():\n",
    "            del cols[3]\n",
    "\n",
    "        features = reader.get_features()\n",
    "        self.scaler.fit(data[:, cols])\n",
    "        \n",
    "        for clients in [10]:\n",
    "            self.CLIENTS = clients\n",
    "            if ((self.file == \"App_usage_trace.txt\") or (self.file == \"top_90_apps.csv\") or (\"infected\" in self.file) or (\"Cosphere\" in self.file)): \n",
    "                # for subsets: renumber the client ids\n",
    "                data  = utils.map_ids(data.copy())\n",
    "                #create list of client ids\n",
    "                num_of_users = int((np.amax(data[:, 0]) + 1))\n",
    "                self.client_ids = list(range(0, num_of_users))\n",
    "                random.shuffle(self.client_ids)\n",
    "                self.client_ids = self.client_ids[:self.CLIENTS]\n",
    "                self.client_ids = sorted(self.client_ids)\n",
    "                print(f\"All client-IDs: {self.client_ids}\")\n",
    "                \n",
    "            if (\"IID_2\" in self.file): \n",
    "                self.client_ids =  [i for i in range(0, self.CLIENTS)]\n",
    "\n",
    "            #federated training: create dataset per client\n",
    "            self.train_data = []\n",
    "            self.test_data = []\n",
    "\n",
    "            for id in self.client_ids:\n",
    "                \n",
    "                indicees = data[:, 0] == id\n",
    "                former_shape = data[indicees, self.start_id:features].shape\n",
    "\n",
    "                #delete index 0 and 3 or 9, containing the label and the user id\n",
    "                client_x = np.delete( data[indicees, self.start_id:features], self.label_id-1, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
    "                #scale \n",
    "                client_x = self.scaler.transform(client_x)\n",
    "                client_y = data[indicees, self.label_id].reshape(-1, 1)\n",
    "\n",
    "                if len(client_x) > 1:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(client_x, client_y, test_size=0.2, random_state=42)    \n",
    "                    ds_train = self.create_dataset(X_train, y_train)\n",
    "                    ds_test = self.create_dataset(X_test, y_test)\n",
    "                    print(\"Client {}: Created  dataset\".format(id))\n",
    "\n",
    "                    self.train_data.append(ds_train)\n",
    "                    self.test_data.append(ds_test)\n",
    "                else:\n",
    "                    print(\"Could not generate datasets for client {} as there is just one entry in X_train\".format(id))\n",
    "                    self.client_ids.remove(id)\n",
    "\n",
    "            # Check format for TFF: needs to be in shape(None, dim)\n",
    "            #like eg:\n",
    "            # (TensorSpec(shape=(None, 3), dtype=tf.float64, name=None),\n",
    "            #  TensorSpec(shape=(None, 1), dtype=tf.float64, name=None)\n",
    "\n",
    "            #Check format for unfederated Learning: shape(batchsize, dim)\n",
    "            print(self.train_data[0].element_spec)\n",
    "            print(self.test_data[0].element_spec)\n",
    "\n",
    "            self.tff_utils = TFF_Utils()\n",
    "\n",
    "            #run without batch normalization\n",
    "            self.use_tff = True\n",
    "            self.use_bn = False\n",
    "            self.compression = False\n",
    "            self.init_mlflow()\n",
    "            self.run_federated()\n",
    "            \n",
    "            #run with batch normalization\n",
    "            self.use_bn = True\n",
    "            self.init_mlflow()\n",
    "            self.run_federated()\n",
    "            \n",
    "            #test compression\n",
    "            for t in [10000, 1499, 500]:\n",
    "                print(f\"--------\\nClient {clients} mit Threshold: {t}--------\")\n",
    "                self.quantization_thresh = t\n",
    "                \n",
    "                #run without batch normalization            \n",
    "                self.compression = True\n",
    "                self.use_bn = False\n",
    "                self.init_mlflow()\n",
    "                self.run_federated()\n",
    "                \n",
    "                #run with batch normalization\n",
    "                self.use_bn = True\n",
    "                self.init_mlflow()\n",
    "                self.run_federated()\n",
    "\n",
    "            #run zentralized\n",
    "            if compare_centralized:\n",
    "\n",
    "                self.use_tff = False\n",
    "                self.use_bn = False\n",
    "                self.compression = False\n",
    "                self.init_mlflow()\n",
    "\n",
    "                #get same client ids, as with tff\n",
    "                mask = np.isin(data[:, 0], self.client_ids)\n",
    "                x = data[mask].copy() \n",
    "                #create dataset for centralized training\n",
    "                unfederated_train, unfederated_test, unfederated_val = self.create_unfederated_dataset(x, reader.get_features())\n",
    "                self.run_unfederated(unfederated_train, unfederated_test, unfederated_val,  (reader.get_features()-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Start training with different datasets\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# name of different input files and unique labels\n",
    "input_options = {    \"0\" : (\"App_usage_trace.txt\", 3996),\n",
    "                     \"1\" : (\"top_90_apps.csv\" , 3996), \n",
    "                     \"2\" : (\"10_infected.csv\", 3),\n",
    "                     \"3\" : (\"num_classes_infected_shuffled.csv\", 3),\n",
    "                     \"4\" : (\"Cosphere.csv\", 9972),\n",
    "                     \"5\" : (\"Cosphere_cropped.csv\", 9972),\n",
    "                     \"6\" : (\"top_90_apps_IID_2.csv\", 3996)\n",
    "                }\n",
    "#specify which dataset to use\n",
    "used_option = \"1\"\n",
    "c_vs_fed_trainer = Centralized_vs_Federated_Trainer(file_name = input_options[used_option][0], \n",
    "                                                    number_of_classes = input_options[used_option][1],\n",
    "                                                    E = 1, \n",
    "                                                    compression = True)\n",
    "# run\n",
    "c_vs_fed_trainer.main(compare_centralized=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FL_vs_Non_FL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
