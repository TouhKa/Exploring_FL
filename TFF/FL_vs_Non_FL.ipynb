{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FL_vs_Non_FL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM4kVhzbR_UK"
      },
      "source": [
        "!pip install --upgrade tensorflow-federated\n",
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbo1O-krQDQh"
      },
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    sys.path.append(\"./drive/MyDrive/Colab Notebooks/Projektarbeit_2/\")\n",
        "    BASE_DIR = sys.path[-1]\n",
        "    !pip install --upgrade tensorflow-federated\n",
        "    !pip install nest_asyncio\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    # from google.colab import files\n",
        "    # files.upload()\n",
        "\n",
        "else: \n",
        "    BASE_DIR = \"../\"\n",
        "    sys.path.append(BASE_DIR)\n",
        "\n",
        "import json\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "from tensorflow.keras import Model, callbacks\n",
        "from tensorflow.keras.layers import Dense, Softmax\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import random\n",
        "import time\n",
        "from Reader import Reader\n",
        "from FLModel import FLModel\n",
        "from Utils import Utils\n",
        "import statistics\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVvL_MUFQ1zV"
      },
      "source": [
        "def read_config():\n",
        "    config_file = BASE_DIR + \"config/config.json\"\n",
        "    config = None\n",
        "    with open(config_file) as json_file:\n",
        "        config = json.loads(json_file.read())\n",
        "    return config\n",
        "\n",
        "def split_input_target(input, target):\n",
        "    return input, target\n",
        "\n",
        "def create_dataset(x, y, use_tff = True):\n",
        "    ds =  tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    \n",
        "    if use_tff:\n",
        "        return (\n",
        "        ds.repeat(EPOCHS).shuffle(SHUFFLE_BUFFER)\n",
        "        .map(split_input_target)).batch(BATCH_SIZE) \n",
        "    else:\n",
        "        return ds.repeat(BATCH_SIZE).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE,drop_remainder = True) \n",
        "\n",
        "def get_split(x, y):\n",
        "    return train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def create_unfederated_dataset(x, features):\n",
        "    former_shape = x[:, 1:features].shape\n",
        "    client_x = np.delete( x[:, 1:features], 2, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
        "    client_x = scaler.transform(client_x)\n",
        "    client_y = x[:, 3].reshape(-1, 1)\n",
        "    X_train, X_test, y_train, y_test = get_split(client_x, client_y)\n",
        "    X_train, X_val, y_train, y_val = get_split(X_train, y_train)\n",
        "    train_data = create_dataset(X_train, y_train, use_tff=False)\n",
        "    test_data = create_dataset(X_test, y_test, use_tff=False)\n",
        "    val_data = create_dataset(X_val, y_val, use_tff=False)\n",
        "    return train_data, test_data, val_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbOS_rnORDWI"
      },
      "source": [
        "config = read_config()\n",
        "BATCH_SIZE = config[\"BATCH_SIZE\"]\n",
        "PREFETCH_BUFFER = config[\"PREFETCH_BUFFER\"]\n",
        "SHUFFLE_BUFFER = config[\"SHUFFLE_BUFFER\"]\n",
        "CLIENTS = config[\"CLIENTS\"]\n",
        "DATA_DIR = config[\"DATA_DIR\"]\n",
        "OUT_DIR = config[\"OUT_DIR\"]\n",
        "LOG_DIR = config[\"LOG_DIR\"]\n",
        "EPOCHS = config[\"EPOCHS\"] \n",
        "NUM_CLASSES = config[\"NUM_CLASSES\"]\n",
        "file = config[\"file_top_apps\"]\n",
        "if IN_COLAB:\n",
        "    tf_log_dir = \"/tmp/logs/scalars/tf_training/\"\n",
        "    !rm -R /tmp/logs/scalars/*\n",
        "\n",
        "else:\n",
        "    tf_log_dir = LOG_DIR + \"tensorboard/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNFKDsaxVUVx"
      },
      "source": [
        "k = 10\n",
        "entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "sparseCategoricalAcc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "sparseTopKCategoricalAccuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=k)\n",
        "client_optimizer =  tf.keras.optimizers.SGD(learning_rate= 0.6, momentum=0.6, nesterov=True)\n",
        "server_optimzer = tf.keras.optimizers.SGD(learning_rate= 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Mh5UtwUTKk"
      },
      "source": [
        "# from client_manager\n",
        "train_data = []\n",
        "test_data = []\n",
        "client_ids = None\n",
        "use_tff = False\n",
        "scaler = StandardScaler()\n",
        "utils = Utils()\n",
        "reader = Reader(BASE_DIR + DATA_DIR, file)\n",
        "data = reader.get_data()\n",
        "\n",
        "if (\"IID\" in file): \n",
        "    data = utils.create_clients(data, CLIENTSs, strict = False)\n",
        "    reader.set_features(reader.get_features() + 1)\n",
        "    client_ids =  [i for i in range(0, CLIENTS)]\n",
        "cols = [i for i in range(0, reader.get_features())]\n",
        "del cols[0]\n",
        "del cols[2]\n",
        "\n",
        "features = reader.get_features()\n",
        "scaler.fit(data[:, cols])\n",
        "if ((file == \"App_usage_trace.txt\") or (file == \"top_90_apps.csv\")): \n",
        "    data  = utils.map_ids(data.copy())\n",
        "    num_of_users = int((np.amax(data[:, 0]) + 1))\n",
        "    client_ids = list(range(0, num_of_users))\n",
        "    random.shuffle(client_ids)\n",
        "    client_ids = client_ids[:CLIENTS]\n",
        "client_ids = sorted(client_ids)\n",
        "for id in client_ids:\n",
        "    indicees = data[:, 0] == id\n",
        "    former_shape = data[indicees, 1:features].shape\n",
        "    #delete index 0 and 3, containing the label and the user id\n",
        "    client_x = np.delete( data[indicees, 1:features], 2, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
        "    #scale \n",
        "    client_x = scaler.transform(client_x)\n",
        "    client_y = data[indicees, 3].reshape(-1, 1)\n",
        "    if len(client_x) > 1:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(client_x, client_y, test_size=0.2, random_state=42)    \n",
        "        ds_train = create_dataset(X_train, y_train, use_tff)\n",
        "        ds_test = create_dataset(X_test, y_test, use_tff)\n",
        "        print(\"Client {}: Created  dataset\".format(id))\n",
        "\n",
        "        train_data.append(ds_train)\n",
        "        test_data.append(ds_test)\n",
        "    else:\n",
        "        print(\"Could not generate datasets for client {} as there is just one entry in X_train\".format(id))\n",
        "        client_ids.remove(id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVFNL9q3N9yQ"
      },
      "source": [
        "# Check format for TFF: needs to be in shape(None, dim)\n",
        "#like eg:\n",
        "# (TensorSpec(shape=(None, 3), dtype=tf.float64, name=None),\n",
        "#  TensorSpec(shape=(None, 1), dtype=tf.float64, name=None)\n",
        "print(train_data[0].element_spec)\n",
        "print(test_data[0].element_spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMMT1uVrUnuq"
      },
      "source": [
        "def create_keras_model():\n",
        "    return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "      tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
        "      tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "# Each time the next method is called, the server model is broadcast to each client using a broadcast function. \n",
        "# For each client, one epoch of local training is performed via the tf.keras.optimizers.Optimizer.apply_gradients method of the client optimizer. \n",
        "# Each client computes the difference between the client model after training and the initial broadcast model. \n",
        "# These model deltas are then aggregated at the server using some aggregation function. \n",
        "# The aggregate model delta is applied at the server by using the tf.keras.optimizers.Optimizer.apply_gradients method of the server optimizer.\n",
        "def model_fn():\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "    keras_model = create_keras_model()\n",
        "    return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec = train_data[0].element_spec,\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), \n",
        "               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=k)]) \n",
        "  \n",
        "def run_federated():\n",
        "    with tf.device('/gpu:0'):\n",
        "        iterative_process = tff.learning.build_federated_averaging_process(\n",
        "            model_fn,\n",
        "            client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= 0.6, momentum=0.6, nesterov=True), \n",
        "            server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= 1.0)\n",
        "            )\n",
        "\n",
        "    state = iterative_process.initialize()\n",
        "    with summary_writer.as_default():\n",
        "        for round_num in range(EPOCHS):\n",
        "            state, metrics = iterative_process.next(state, train_data)\n",
        "\n",
        "            # Note: training metrics reported by the iterative training process \n",
        "            #generally reflect the performance of the model at the beginning of the training round\n",
        "            for name, value in metrics['train'].items():\n",
        "                tf.summary.scalar(name, value, step=round_num)\n",
        "                print(round_num, name, value)\n",
        "\n",
        "            evaluation = tff.learning.build_federated_evaluation(model_fn)  \n",
        "            test_metrics = evaluation(state.model, test_data)\n",
        "            print(f\"{round_num} Test: {test_metrics}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77RS9R1uU1WY"
      },
      "source": [
        "summary_writer = tf.summary.create_file_writer(tf_log_dir + \"federated/\")\n",
        "run_federated()\n",
        "%tensorboard --logdir {tf_log_dir + \"federated/\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRXvHm1t3NJO"
      },
      "source": [
        "## Unfederated Trainings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX2J_ediZLmY"
      },
      "source": [
        "def run_unfederated(ds_train, ds_test, ds_val, input_dim):\n",
        "\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                min_delta=0.01, \n",
        "                                patience=2, \n",
        "                                verbose=0, \n",
        "                                mode='auto', \n",
        "                                baseline=None, \n",
        "                                restore_best_weights=True)\n",
        "\n",
        "    model = FLModel(NUM_CLASSES)\n",
        "    model.compile(\n",
        "                optimizer= client_optimizer, \n",
        "                loss= \"sparse_categorical_crossentropy\", \n",
        "                metrics= [\n",
        "                          sparseCategoricalAcc, \n",
        "                          sparseTopKCategoricalAccuracy\n",
        "                          ]\n",
        "                )\n",
        "    for epoch in range(EPOCHS):\n",
        "        with tf.device('/gpu:0'):\n",
        "            history = model.fit(\n",
        "                              ds_train,\n",
        "                              steps_per_epoch=64, \n",
        "                              validation_data = ds_val, \n",
        "                              verbose=0,\n",
        "                              callbacks = [early_stopping_callback])\n",
        "\n",
        "        loss = round(history.history[\"loss\"][0], 4)\n",
        "        acc = round(history.history[\"sparse_categorical_accuracy\"][0], 4)\n",
        "        k_acc =  round(history.history[\"sparse_top_k_categorical_accuracy\"][0], 4)\n",
        "        val_loss =  round(history.history['val_loss'][0], 4)\n",
        "        val_acc = round(history.history['val_sparse_categorical_accuracy'][0], 4)\n",
        "        val_k_acc =  round(history.history['val_sparse_top_k_categorical_accuracy'][0], 4)\n",
        "\n",
        "        with tf.device('/gpu:0'):\n",
        "            test_loss, test_acc, test_k_acc = model.evaluate(ds_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "        \n",
        "        test_loss = round(test_loss, 4)\n",
        "        test_acc = round(test_acc, 4)\n",
        "        test_k_acc = round(test_k_acc, 4)\n",
        "\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar(\"Loss/train\", loss, step=epoch)\n",
        "            tf.summary.scalar(\"Acc/train\", acc, step=epoch)\n",
        "            tf.summary.scalar(\"K_acc/train\", k_acc, step=epoch)\n",
        "\n",
        "            tf.summary.scalar(\"Loss/validation\", val_loss, step=epoch)\n",
        "            tf.summary.scalar(\"Acc/validation\", val_acc, step=epoch)\n",
        "            tf.summary.scalar(\"K_acc/validation\", val_k_acc, step=epoch)\n",
        "\n",
        "            tf.summary.scalar(\"Loss/test\", test_loss, step=epoch)\n",
        "            tf.summary.scalar(\"Acc/test\", test_acc, step=epoch)\n",
        "            tf.summary.scalar(\"K_acc/test\", test_k_acc, step=epoch)\n",
        "\n",
        "        print(\n",
        "          f'Epoch: {epoch},\\n'\n",
        "          f'Train Loss:\\t{loss}, '\n",
        "          f'Train Accuracy:\\t{acc}, '\n",
        "          f'Train Top 5 Accuracy:\\t{k_acc}\\n'\n",
        "          f'Validation Loss:\\t{val_loss}, '\n",
        "          f'Validation Accuracy:\\t{val_acc}, '\n",
        "          f'Validation Top 5 Accuracy:\\t{val_k_acc}\\n'\n",
        "          f'Test Loss:\\t{test_loss}, '\n",
        "          f'Test Accuracy:\\t{test_acc} '\n",
        "          f'Test Top 5 Accuracy:\\t{test_k_acc}'\n",
        "          f'\\n--------------------------------------------------------------------------------------------------------------------------\\n'\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag6lU8AFOw6m"
      },
      "source": [
        "# !rm -R \"/tmp/logs/scalars/tf_training/unfederated/\"\n",
        "summary_writer = tf.summary.create_file_writer(tf_log_dir + \"unfederated/\")\n",
        "\n",
        "#get same client ids, as with tff\n",
        "mask = np.isin(data[:, 0], client_ids)\n",
        "x = data[mask].copy() \n",
        "\n",
        "unfederated_train, unfederated_test, unfederated_val = create_unfederated_dataset(x, reader.get_features())\n",
        "run_unfederated(unfederated_train, unfederated_test, unfederated_val,  (reader.get_features()-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNT_g99NVyBm"
      },
      "source": [
        "%tensorboard --logdir {tf_log_dir + \"unfederated/\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rInfrkwfVyBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}