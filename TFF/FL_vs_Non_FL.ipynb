{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lM4kVhzbR_UK",
    "outputId": "a0ff8267-c327-408a-8dff-b8a7351eceb8"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow-federated\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vbo1O-krQDQh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    sys.path.append(\"./drive/MyDrive/Colab Notebooks/Projektarbeit_2/\")\n",
    "    BASE_DIR = sys.path[-1]\n",
    "    !pip install --upgrade tensorflow-federated\n",
    "    !pip install nest_asyncio\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    # from google.colab import files\n",
    "    # files.upload()\n",
    "\n",
    "else: \n",
    "    BASE_DIR = \"../\"\n",
    "    sys.path.append(BASE_DIR)\n",
    "\n",
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# import tensorflow_federated as tff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from tensorflow.keras import Model, callbacks\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import random\n",
    "import time\n",
    "from Reader import Reader\n",
    "from FLModel import FLModel\n",
    "from Utils import Utils\n",
    "import statistics\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fVvL_MUFQ1zV"
   },
   "outputs": [],
   "source": [
    "def read_config():\n",
    "    config_file = BASE_DIR + \"config/config.json\"\n",
    "    config = None\n",
    "    with open(config_file) as json_file:\n",
    "        config = json.loads(json_file.read())\n",
    "    return config\n",
    "\n",
    "def split_input_target(input, target):\n",
    "    return input, target\n",
    "\n",
    "def create_dataset(x, y, use_tff = True):\n",
    "    ds =  tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    if use_tff:\n",
    "        return (\n",
    "        ds.repeat(EPOCHS).shuffle(SHUFFLE_BUFFER)\n",
    "        .map(split_input_target)).batch(BATCH_SIZE) \n",
    "    else:\n",
    "        return ds.repeat(BATCH_SIZE).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE,drop_remainder = True) \n",
    "\n",
    "def get_split(x, y):\n",
    "    return train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_unfederated_dataset(x, features):\n",
    "    former_shape = x[:, 1:features].shape\n",
    "    client_x = np.delete( x[:, 1:features], 2, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
    "    client_x = scaler.transform(client_x)\n",
    "    client_y = x[:, 3].reshape(-1, 1)\n",
    "    X_train, X_test, y_train, y_test = get_split(client_x, client_y)\n",
    "    X_train, X_val, y_train, y_val = get_split(X_train, y_train)\n",
    "    train_data = create_dataset(X_train, y_train, use_tff=False)\n",
    "    test_data = create_dataset(X_test, y_test, use_tff=False)\n",
    "    val_data = create_dataset(X_val, y_val, use_tff=False)\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbOS_rnORDWI",
    "outputId": "3adc93f4-b034-4024-931d-1238b822321e"
   },
   "outputs": [],
   "source": [
    "config = read_config()\n",
    "BATCH_SIZE = config[\"BATCH_SIZE\"]\n",
    "PREFETCH_BUFFER = config[\"PREFETCH_BUFFER\"]\n",
    "SHUFFLE_BUFFER = config[\"SHUFFLE_BUFFER\"]\n",
    "CLIENTS = config[\"CLIENTS\"]\n",
    "DATA_DIR = config[\"DATA_DIR\"]\n",
    "OUT_DIR = config[\"OUT_DIR\"]\n",
    "LOG_DIR = config[\"LOG_DIR\"]\n",
    "EPOCHS = config[\"EPOCHS\"] \n",
    "NUM_CLASSES = config[\"NUM_CLASSES\"]\n",
    "file = config[\"file_top_apps\"]\n",
    "if IN_COLAB:\n",
    "    tf_log_dir = \"/tmp/logs/scalars/tf_training/\"\n",
    "    !rm -R /tmp/logs/scalars/*\n",
    "\n",
    "else:\n",
    "    tf_log_dir = LOG_DIR + \"tensorboard/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BNFKDsaxVUVx"
   },
   "outputs": [],
   "source": [
    "entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "sparseCategoricalAcc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "sparseTopKCategoricalAccuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)\n",
    "client_optimizer =  tf.keras.optimizers.SGD(learning_rate= 0.6, momentum=0.6, nesterov=True)\n",
    "server_optimzer = tf.keras.optimizers.SGD(learning_rate= 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96Mh5UtwUTKk",
    "outputId": "00c57841-e59d-4393-8f79-96368324af15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 43: Created  dataset\n",
      "Client 285: Created  dataset\n",
      "Client 389: Created  dataset\n",
      "Client 448: Created  dataset\n",
      "Client 495: Created  dataset\n",
      "Client 499: Created  dataset\n",
      "Client 544: Created  dataset\n",
      "Client 602: Created  dataset\n",
      "Client 719: Created  dataset\n",
      "Client 808: Created  dataset\n"
     ]
    }
   ],
   "source": [
    "# from client_manager\n",
    "train_data = []\n",
    "test_data = []\n",
    "client_ids = None\n",
    "use_tff = False\n",
    "scaler = StandardScaler()\n",
    "utils = Utils()\n",
    "reader = Reader(BASE_DIR + DATA_DIR, file)\n",
    "data = reader.get_data()\n",
    "\n",
    "if (\"IID\" in file): \n",
    "    data = utils.create_clients(data, CLIENTSs, strict = False)\n",
    "    reader.set_features(reader.get_features() + 1)\n",
    "    client_ids =  [i for i in range(0, CLIENTS)]\n",
    "cols = [i for i in range(0, reader.get_features())]\n",
    "del cols[0]\n",
    "del cols[2]\n",
    "\n",
    "features = reader.get_features()\n",
    "scaler.fit(data[:, cols])\n",
    "if ((file == \"App_usage_trace.txt\") or (file == \"top_90_apps.csv\")): \n",
    "    data  = utils.map_ids(data.copy())\n",
    "    num_of_users = int((np.amax(data[:, 0]) + 1))\n",
    "    client_ids = list(range(0, num_of_users))\n",
    "    random.shuffle(client_ids)\n",
    "    client_ids = client_ids[:CLIENTS]\n",
    "client_ids = sorted(client_ids)\n",
    "for id in client_ids:\n",
    "    indicees = data[:, 0] == id\n",
    "    former_shape = data[indicees, 1:features].shape\n",
    "    #delete index 0 and 3, containing the label and the user id\n",
    "    client_x = np.delete( data[indicees, 1:features], 2, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
    "    #scale \n",
    "    client_x = scaler.transform(client_x)\n",
    "    client_y = data[indicees, 3].reshape(-1, 1)\n",
    "    if len(client_x) > 1:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(client_x, client_y, test_size=0.2, random_state=42)    \n",
    "        ds_train = create_dataset(X_train, y_train, use_tff)\n",
    "        ds_test = create_dataset(X_test, y_test, use_tff)\n",
    "        print(\"Client {}: Created  dataset\".format(id))\n",
    "\n",
    "        train_data.append(ds_train)\n",
    "        test_data.append(ds_test)\n",
    "    else:\n",
    "        print(\"Could not generate datasets for client {} as there is just one entry in X_train\".format(id))\n",
    "        client_ids.remove(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVFNL9q3N9yQ",
    "outputId": "e77ed011-3235-4487-dd56-9e1b45d3acac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(64, 3), dtype=tf.float64, name=None), TensorSpec(shape=(64, 1), dtype=tf.float64, name=None))\n",
      "(TensorSpec(shape=(64, 3), dtype=tf.float64, name=None), TensorSpec(shape=(64, 1), dtype=tf.float64, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Check format for TFF: needs to be in shape(None, dim)\n",
    "#like eg:\n",
    "# (TensorSpec(shape=(None, 3), dtype=tf.float64, name=None),\n",
    "#  TensorSpec(shape=(None, 1), dtype=tf.float64, name=None)\n",
    "print(train_data[0].element_spec)\n",
    "print(test_data[0].element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMMT1uVrUnuq"
   },
   "outputs": [],
   "source": [
    "def create_keras_model(input_dim=3):\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "      tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "# Each time the next method is called, the server model is broadcast to each client using a broadcast function. \n",
    "# For each client, one epoch of local training is performed via the tf.keras.optimizers.Optimizer.apply_gradients method of the client optimizer. \n",
    "# Each client computes the difference between the client model after training and the initial broadcast model. \n",
    "# These model deltas are then aggregated at the server using some aggregation function. \n",
    "# The aggregate model delta is applied at the server by using the tf.keras.optimizers.Optimizer.apply_gradients method of the server optimizer.\n",
    "def model_fn():\n",
    "  # We _must_ create a new model here, and _not_ capture it from an external\n",
    "  # scope. TFF will call this within different graph contexts.\n",
    "    keras_model = create_keras_model(3)\n",
    "    return tff.learning.from_keras_model(\n",
    "      keras_model,\n",
    "      input_spec = train_data[0].element_spec,\n",
    "      loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]) \n",
    "  \n",
    "def run_federated():\n",
    "    with tf.device('/gpu:0'):\n",
    "        iterative_process = tff.learning.build_federated_averaging_process(\n",
    "            model_fn,\n",
    "            client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= 0.6, momentum=0.6, nesterov=True), \n",
    "            server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= 1.0)\n",
    "            )\n",
    "\n",
    "    state = iterative_process.initialize()\n",
    "    with summary_writer.as_default():\n",
    "        for round_num in range(EPOCHS):\n",
    "            state, metrics = iterative_process.next(state, train_data)\n",
    "\n",
    "            # Note: training metrics reported by the iterative training process \n",
    "            #generally reflect the performance of the model at the beginning of the training round\n",
    "            for name, value in metrics['train'].items():\n",
    "                tf.summary.scalar(name, value, step=round_num)\n",
    "                print(round_num, name, value)\n",
    "\n",
    "            evaluation = tff.learning.build_federated_evaluation(model_fn)  \n",
    "            test_metrics = evaluation(state.model, test_data)\n",
    "            print(f\"{round_num} Test: {test_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77RS9R1uU1WY"
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(tf_log_dir + \"federated/\")\n",
    "run_federated()\n",
    "%tensorboard --logdir {tf_log_dir + \"federated/\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRXvHm1t3NJO"
   },
   "source": [
    "## Unfederated Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dX2J_ediZLmY"
   },
   "outputs": [],
   "source": [
    "def run_unfederated(ds_train, ds_test, ds_val, input_dim):\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                min_delta=0.01, \n",
    "                                patience=2, \n",
    "                                verbose=0, \n",
    "                                mode='auto', \n",
    "                                baseline=None, \n",
    "                                restore_best_weights=True)\n",
    "\n",
    "    model = FLModel(NUM_CLASSES)\n",
    "    model.compile(\n",
    "                optimizer= client_optimizer, \n",
    "                loss= \"sparse_categorical_crossentropy\", \n",
    "                metrics= [\n",
    "                          sparseCategoricalAcc, \n",
    "                          sparseTopKCategoricalAccuracy\n",
    "                          ]\n",
    "                )\n",
    "    for epoch in range(EPOCHS):\n",
    "        with tf.device('/gpu:0'):\n",
    "            history = model.fit(\n",
    "                              ds_train,\n",
    "                              steps_per_epoch=64, \n",
    "                              validation_data = ds_val, \n",
    "                              verbose=0,\n",
    "                              callbacks = [early_stopping_callback])\n",
    "\n",
    "        loss = round(history.history[\"loss\"][0], 4)\n",
    "        acc = round(history.history[\"sparse_categorical_accuracy\"][0], 4)\n",
    "        k_acc =  round(history.history[\"sparse_top_k_categorical_accuracy\"][0], 4)\n",
    "        val_loss =  round(history.history['val_loss'][0], 4)\n",
    "        val_acc = round(history.history['val_sparse_categorical_accuracy'][0], 4)\n",
    "        val_k_acc =  round(history.history['val_sparse_top_k_categorical_accuracy'][0], 4)\n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "            test_loss, test_acc, test_k_acc = model.evaluate(ds_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "        \n",
    "        test_loss = round(test_loss, 4)\n",
    "        test_acc = round(test_acc, 4)\n",
    "        test_k_acc = round(test_k_acc, 4)\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Loss/train\", loss, step=epoch)\n",
    "            tf.summary.scalar(\"Acc/train\", acc, step=epoch)\n",
    "            tf.summary.scalar(\"K_acc/train\", k_acc, step=epoch)\n",
    "\n",
    "            tf.summary.scalar(\"Loss/validation\", val_loss, step=epoch)\n",
    "            tf.summary.scalar(\"Acc/validation\", val_acc, step=epoch)\n",
    "            tf.summary.scalar(\"K_acc/validation\", val_k_acc, step=epoch)\n",
    "\n",
    "            tf.summary.scalar(\"Loss/test\", test_loss, step=epoch)\n",
    "            tf.summary.scalar(\"Acc/test\", test_acc, step=epoch)\n",
    "            tf.summary.scalar(\"K_acc/test\", test_k_acc, step=epoch)\n",
    "\n",
    "        print(\n",
    "          f'Epoch: {epoch},\\n'\n",
    "          f'Train Loss:\\t{loss}, '\n",
    "          f'Train Accuracy:\\t{acc}, '\n",
    "          f'Train Top 5 Accuracy:\\t{k_acc}\\n'\n",
    "          f'Validation Loss:\\t{val_loss}, '\n",
    "          f'Validation Accuracy:\\t{val_acc}, '\n",
    "          f'Validation Top 5 Accuracy:\\t{val_k_acc}\\n'\n",
    "          f'Test Loss:\\t{test_loss}, '\n",
    "          f'Test Accuracy:\\t{test_acc} '\n",
    "          f'Test Top 5 Accuracy:\\t{test_k_acc}'\n",
    "          f'\\n--------------------------------------------------------------------------------------------------------------------------\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ag6lU8AFOw6m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer fl_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer fl_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0,\n",
      "Train Loss: 5.8816, Train Accuracy: 0.1948, Train Top 5 Accuracy: 0.53\n",
      "Validation Loss: 4.8007, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 4.873, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 1,\n",
      "Train Loss: 4.5695, Train Accuracy: 0.1982, Train Top 5 Accuracy: 0.5269\n",
      "Validation Loss: 4.2695, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 4.3255, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 2,\n",
      "Train Loss: 4.2017, Train Accuracy: 0.197, Train Top 5 Accuracy: 0.5247\n",
      "Validation Loss: 4.0276, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 4.0766, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 3,\n",
      "Train Loss: 3.9875, Train Accuracy: 0.1985, Train Top 5 Accuracy: 0.5278\n",
      "Validation Loss: 3.8712, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 3.9202, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 4,\n",
      "Train Loss: 3.8556, Train Accuracy: 0.1946, Train Top 5 Accuracy: 0.5281\n",
      "Validation Loss: 3.7617, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 3.8115, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 5,\n",
      "Train Loss: 3.7629, Train Accuracy: 0.1995, Train Top 5 Accuracy: 0.5261\n",
      "Validation Loss: 3.6812, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 3.7337, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 6,\n",
      "Train Loss: 3.6895, Train Accuracy: 0.1992, Train Top 5 Accuracy: 0.5264\n",
      "Validation Loss: 3.6221, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 3.6766, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 7,\n",
      "Train Loss: 3.6335, Train Accuracy: 0.197, Train Top 5 Accuracy: 0.5269\n",
      "Validation Loss: 3.5791, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 3.6333, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 8,\n",
      "Train Loss: 3.5937, Train Accuracy: 0.1985, Train Top 5 Accuracy: 0.5266\n",
      "Validation Loss: 3.5452, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 3.599, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 9,\n",
      "Train Loss: 3.5633, Train Accuracy: 0.1975, Train Top 5 Accuracy: 0.5271\n",
      "Validation Loss: 3.5147, Validation Accuracy: 0.2307, Validation Top 5 Accuracy: 0.5406\n",
      "Test Loss: 3.5701, Test Accuracy: 0.2088 Test Top 5 Accuracy: 0.5281\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !rm -R \"/tmp/logs/scalars/tf_training/unfederated/\"\n",
    "summary_writer = tf.summary.create_file_writer(tf_log_dir + \"unfederated/\")\n",
    "\n",
    "#get same client ids, as with tff\n",
    "mask = np.isin(data[:, 0], client_ids)\n",
    "x = data[mask].copy() \n",
    "\n",
    "unfederated_train, unfederated_test, unfederated_val = create_unfederated_dataset(x, reader.get_features())\n",
    "run_unfederated(unfederated_train, unfederated_test, unfederated_val,  (reader.get_features()-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9940), started 0:06:13 ago. (Use '!kill 9940' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f8aaefcf382483f8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f8aaefcf382483f8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {tf_log_dir + \"unfederated/\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FL_vs_Non_FL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
