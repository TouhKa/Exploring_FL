{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM4kVhzbR_UK"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow-federated\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbo1O-krQDQh"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    sys.path.append(\"./drive/MyDrive/Colab Notebooks/Projektarbeit_2/\")\n",
    "    BASE_DIR = sys.path[-1]\n",
    "    !pip install --upgrade tensorflow-federated\n",
    "    !pip install nest_asyncio\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "else: \n",
    "    BASE_DIR = \"../\"\n",
    "    sys.path.append(BASE_DIR)\n",
    "\n",
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from tensorflow.keras import Model, callbacks\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import random\n",
    "import time\n",
    "from Reader import Reader\n",
    "from model.FLModel import FLModel\n",
    "from model.BNModel import BNModel\n",
    "from Utils import Utils\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVvL_MUFQ1zV"
   },
   "outputs": [],
   "source": [
    "def read_config():\n",
    "    config_file = BASE_DIR + \"config/config.json\"\n",
    "    config = None\n",
    "    with open(config_file) as json_file:\n",
    "        config = json.loads(json_file.read())\n",
    "    return config\n",
    "\n",
    "def split_input_target(input, target):\n",
    "    return input, target\n",
    "\n",
    "def create_dataset(x, y, use_tff = True):\n",
    "    ds =  tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    if use_tff:\n",
    "        return (\n",
    "        ds.repeat(EPOCHS).shuffle(SHUFFLE_BUFFER)\n",
    "        .map(split_input_target)).batch(BATCH_SIZE) \n",
    "    else:\n",
    "        return ds.repeat(BATCH_SIZE * EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE,drop_remainder = True) \n",
    "\n",
    "def get_split(x, y):\n",
    "    return train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_unfederated_dataset(x, features):\n",
    "    former_shape = x[:, start_id:features].shape\n",
    "    client_x = np.delete( x[:, start_id:features], label_id-1, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
    "    client_x = scaler.transform(client_x)\n",
    "    client_y = x[:, label_id].reshape(-1, 1)\n",
    "    X_train, X_test, y_train, y_test = get_split(client_x, client_y)\n",
    "    X_train, X_val, y_train, y_val = get_split(X_train, y_train)\n",
    "    train_data = create_dataset(X_train, y_train, use_tff=False)\n",
    "    test_data = create_dataset(X_test, y_test, use_tff=False)\n",
    "    val_data = create_dataset(X_val, y_val, use_tff=False)\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_size(size):\n",
    "  \"\"\"A helper function for creating a human-readable size.\"\"\"\n",
    "  size = float(size)\n",
    "  for unit in ['bit','Kibit','Mibit','Gibit']:\n",
    "    if size < 1024.0:\n",
    "      return \"{size:3.2f}{unit}\".format(size=size, unit=unit)\n",
    "    size /= 1024.0\n",
    "  return \"{size:.2f}{unit}\".format(size=size, unit='TiB')\n",
    "\n",
    "def set_sizing_environment():\n",
    "  \"\"\"Creates an environment that contains sizing information.\"\"\"\n",
    "  # Creates a sizing executor factory to output communication cost\n",
    "  # after the training finishes. Note that sizing executor only provides an\n",
    "  # estimate (not exact) of communication cost, and doesn't capture cases like\n",
    "  # compression of over-the-wire representations. However, it's perfect for\n",
    "  # demonstrating the effect of compression in this tutorial.\n",
    "  sizing_factory = tff.framework.sizing_executor_factory()\n",
    "\n",
    "  # TFF has a modular runtime you can configure yourself for various\n",
    "  # environments and purposes, and this example just shows how to configure one\n",
    "  # part of it to report the size of things.\n",
    "  context = tff.framework.ExecutionContext(executor_fn=sizing_factory)\n",
    "  tff.framework.set_default_context(context)\n",
    "\n",
    "  return sizing_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbOS_rnORDWI"
   },
   "outputs": [],
   "source": [
    "config = read_config()\n",
    "BATCH_SIZE = config[\"BATCH_SIZE\"]\n",
    "PREFETCH_BUFFER = config[\"PREFETCH_BUFFER\"]\n",
    "SHUFFLE_BUFFER = config[\"SHUFFLE_BUFFER\"]\n",
    "CLIENTS = config[\"CLIENTS\"]\n",
    "DATA_DIR = config[\"DATA_DIR\"]\n",
    "OUT_DIR = config[\"OUT_DIR\"]\n",
    "LOG_DIR = config[\"LOG_DIR\"]\n",
    "EPOCHS = config[\"EPOCHS\"] \n",
    "NUM_CLASSES = config[\"num_classes_app_usages\"]\n",
    "file = config[\"file_app_usages\"]\n",
    "drop_index = True\n",
    "if \"infected\" in file:\n",
    "    drop_index = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    tf_log_dir = \"../tmp/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d\") + \"/\"\n",
    "    !rm -R /tmp/logs/scalars/*\n",
    "else:\n",
    "    tf_log_dir = LOG_DIR + \"tensorboard/scalars/\" + datetime.now().strftime(\"%Y%m%d\") + \"/\"\n",
    "    \n",
    "if (\"infected\" in file):\n",
    "    start_id = 1\n",
    "    label_id = 9\n",
    "elif (\"Cosphere\" in file):\n",
    "    start_id = 1\n",
    "    label_id = 4\n",
    "else:\n",
    "    start_id = 1\n",
    "    label_id = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNFKDsaxVUVx"
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "use_bn = True\n",
    "use_tff = True\n",
    "learning_rate  = 1e-1\n",
    "momentum = 0.9\n",
    "nesterov = False\n",
    "\n",
    "entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "sparseCategoricalAcc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "sparseTopKCategoricalAccuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=k)\n",
    "client_optimizer =  tf.keras.optimizers.SGD(learning_rate= learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "server_optimzer = tf.keras.optimizers.SGD(learning_rate= 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96Mh5UtwUTKk"
   },
   "outputs": [],
   "source": [
    "client_ids = None\n",
    "scaler = StandardScaler()\n",
    "utils = Utils()\n",
    "reader = Reader(BASE_DIR + DATA_DIR, file, drop_index)\n",
    "data = reader.get_data()\n",
    "\n",
    "if (\"IID\" in file): \n",
    "    data = utils.create_clients(data, CLIENTSs, strict = False)\n",
    "    reader.set_features(reader.get_features() + 1)\n",
    "    client_ids =  [i for i in range(0, CLIENTS)]\n",
    "    \n",
    "cols = [i for i in range(0, reader.get_features())]\n",
    "del cols[0]\n",
    "if \"app\" in file:\n",
    "    del cols[2]\n",
    "elif \"infected\" in file:\n",
    "    del cols[8]\n",
    "elif \"Cosphere\" in file:\n",
    "    del cols[3]\n",
    "\n",
    "features = reader.get_features()\n",
    "scaler.fit(data[:, cols])\n",
    "\n",
    "if ((file == \"App_usage_trace.txt\") or (file == \"top_90_apps.csv\") or (\"infected\" in file) or (\"Cosphere\" in file)): \n",
    "    data  = utils.map_ids(data.copy())\n",
    "    num_of_users = int((np.amax(data[:, 0]) + 1))\n",
    "    client_ids = list(range(0, num_of_users))\n",
    "    random.shuffle(client_ids)\n",
    "    client_ids = client_ids[:CLIENTS]\n",
    "client_ids = sorted(client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "    \n",
    "for id in client_ids:\n",
    "    indicees = data[:, 0] == id\n",
    "    former_shape = data[indicees, start_id:features].shape\n",
    "    #delete index 0 and 3 or 9, containing the label and the user id\n",
    "    client_x = np.delete( data[indicees, start_id:features], label_id-1, 1 ).reshape(former_shape[0], former_shape[1]-1)\n",
    "    #scale \n",
    "    client_x = scaler.transform(client_x)\n",
    "    client_y = data[indicees, label_id].reshape(-1, 1)\n",
    "    \n",
    "    if len(client_x) > 1:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(client_x, client_y, test_size=0.2, random_state=42)    \n",
    "        ds_train = create_dataset(X_train, y_train, use_tff)\n",
    "        ds_test = create_dataset(X_test, y_test, use_tff)\n",
    "        print(\"Client {}: Created  dataset\".format(id))\n",
    "\n",
    "        train_data.append(ds_train)\n",
    "        test_data.append(ds_test)\n",
    "    else:\n",
    "        print(\"Could not generate datasets for client {} as there is just one entry in X_train\".format(id))\n",
    "        client_ids.remove(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVFNL9q3N9yQ"
   },
   "outputs": [],
   "source": [
    "# Check format for TFF: needs to be in shape(None, dim)\n",
    "#like eg:\n",
    "# (TensorSpec(shape=(None, 3), dtype=tf.float64, name=None),\n",
    "#  TensorSpec(shape=(None, 1), dtype=tf.float64, name=None)\n",
    "\n",
    "#Check format for unfederated Learning: shape(batchsize, dim)\n",
    "print(train_data[0].element_spec)\n",
    "print(test_data[0].element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hZGcZWE-bah"
   },
   "outputs": [],
   "source": [
    "def get_not_bn_idx(trainable_variables):\n",
    "  new_trainable_weights = []\n",
    "  train_var_idx = []\n",
    "  for idx, bn_weights in enumerate(trainable_variables):\n",
    "    if 'batch_normalization' not in bn_weights.name:\n",
    "      train_var_idx.append(idx)\n",
    "  return train_var_idx\n",
    "\n",
    "def get_bn_idx(trainable_variables):\n",
    "  new_trainable_weights = []\n",
    "  train_var_idx = []\n",
    "  for idx, bn_weights in enumerate(trainable_variables):\n",
    "    if 'batch_normalization' in bn_weights.name:\n",
    "      train_var_idx.append(idx)\n",
    "  return train_var_idx\n",
    "\n",
    "def get_weights_by_idx(trainable_variables, var_ids):\n",
    "  new_weights = []  \n",
    "  for i in var_ids:\n",
    "    new_weights.append(trainable_variables[i])\n",
    "  return new_weights\n",
    "\n",
    "def create_keras_model(input_dim):\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "      tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "def create_keras_bn_model(input_dim):\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.BatchNormalization(input_shape=(input_dim,)),\n",
    "      tf.keras.layers.Dense(500, activation=tf.nn.relu),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "# Each time the next method is called, the server model is broadcast to each client using a broadcast function. \n",
    "# For each client, one epoch of local training is performed via the tf.keras.optimizers.Optimizer.apply_gradients method of the client optimizer. \n",
    "# Each client computes the difference between the client model after training and the initial broadcast model. \n",
    "# These model deltas are then aggregated at the server using some aggregation function. \n",
    "# The aggregate model delta is applied at the server by using the tf.keras.optimizers.Optimizer.apply_gradients method of the server optimizer.\n",
    "def model_fn():\n",
    "  # We _must_ create a new model here, and _not_ capture it from an external\n",
    "  # scope. TFF will call this within different graph contexts.\n",
    "    if use_bn:\n",
    "      keras_model = create_keras_bn_model(test_data[0].element_spec[0].shape[1])\n",
    "    else:\n",
    "      keras_model = create_keras_model(test_data[0].element_spec[0].shape[1])\n",
    "    return tff.learning.from_keras_model(\n",
    "      keras_model,\n",
    "      input_spec = train_data[0].element_spec,\n",
    "      loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=k)]) \n",
    "    \n",
    "\n",
    "def map_weights_ids():\n",
    "  model = create_keras_bn_model(train_data[0].element_spec[0].shape[1])\n",
    "  trainable_variables = model.trainable_variables\n",
    "  all_weights_name = []\n",
    "  train_weights_ids = []\n",
    "\n",
    "  for layer in model.layers:\n",
    "    for weight in layer.weights:\n",
    "      all_weights_name.append(weight.name)\n",
    "\n",
    "  for var in trainable_variables:\n",
    "    for idx, name in enumerate(all_weights_name):\n",
    "      if var.name == name:\n",
    "        if 'batch_normalization' not in name:\n",
    "          train_weights_ids.append(idx)\n",
    "\n",
    "  return train_weights_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMMT1uVrUnuq"
   },
   "outputs": [],
   "source": [
    "def run_federated():\n",
    "\n",
    "  # ---custom fed avg implementation for batch normalization-----\n",
    "  # --------------------------START------------------------------\n",
    "  @tff.tf_computation\n",
    "  def server_init():\n",
    "    model = model_fn()\n",
    "    trainable_variables = model.trainable_variables\n",
    "    non_bn_ids = get_not_bn_idx(trainable_variables)\n",
    "    non_bn_weights = get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "    return non_bn_weights\n",
    "\n",
    "  @tf.function\n",
    "  def server_update(model, mean_client_weights):\n",
    "    \"\"\"Updates the server model weights as the average of the client model weights.\"\"\"\n",
    "    updated_model_weights = []\n",
    "    trainable_variables = model.trainable_variables\n",
    "    \n",
    "    non_bn_ids = get_not_bn_idx(trainable_variables)\n",
    "    bn_ids = get_bn_idx(trainable_variables)\n",
    "    non_bn_weights = get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "    bn_weights = get_weights_by_idx(trainable_variables, bn_ids)\n",
    "    \n",
    "    tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                          non_bn_weights, mean_client_weights)\n",
    "    for i in range(len(trainable_variables)):\n",
    "      if i in non_bn_ids:\n",
    "        j = non_bn_ids.index(i)\n",
    "        updated_model_weights.append(non_bn_weights[j])\n",
    "      else: \n",
    "        j = bn_ids.index(i)\n",
    "        updated_model_weights.append(bn_weights[j])\n",
    "    return non_bn_weights\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def client_update(model, dataset, server_weights, client_optimizer):\n",
    "    updated_clients_weights = []\n",
    "    trainable_variables = model.trainable_variables\n",
    "    non_bn_ids = get_not_bn_idx(trainable_variables)\n",
    "    bn_ids = get_bn_idx(trainable_variables)\n",
    "    non_bn_weights = get_weights_by_idx(trainable_variables, non_bn_ids)\n",
    "    bn_weights = get_weights_by_idx(trainable_variables, bn_ids)\n",
    "\n",
    "    # Assign the mean client weights to the server model.\n",
    "    tf.nest.map_structure(lambda x, y: x.assign(y),\n",
    "                          non_bn_weights, server_weights)\n",
    "    \n",
    "    for i in range(len(trainable_variables)):\n",
    "      if i in non_bn_ids:\n",
    "        j = non_bn_ids.index(i)\n",
    "        updated_clients_weights.append(non_bn_weights[j])\n",
    "      else: \n",
    "        j = bn_ids.index(i)\n",
    "        updated_clients_weights.append(bn_weights[j])\n",
    "    \n",
    "    client_weights = updated_clients_weights\n",
    "\n",
    "    for epoch in range(1):\n",
    "      for batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "          outputs = model.forward_pass(batch)\n",
    "        grads = tape.gradient(outputs.loss, client_weights)\n",
    "        grads_and_vars = zip(grads, client_weights)\n",
    "        client_optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    return non_bn_weights\n",
    "\n",
    "  whimsy_model = model_fn()\n",
    "  tf_dataset_type = tff.SequenceType(whimsy_model.input_spec)\n",
    "  model_weights_type = server_init.type_signature.result\n",
    "  federated_server_type = tff.FederatedType(model_weights_type, tff.SERVER)\n",
    "  federated_dataset_type = tff.FederatedType(tf_dataset_type, tff.CLIENTS)\n",
    "  print(federated_server_type)\n",
    "  print(federated_dataset_type)\n",
    "\n",
    "  @tff.tf_computation(tf_dataset_type, model_weights_type)\n",
    "  def client_update_fn(tf_dataset, server_weights):\n",
    "    model = model_fn()\n",
    "    client_optimizer = tf.keras.optimizers.SGD(learning_rate= learning_rate, momentum=momentum, nesterov=nesterov)\n",
    "    return client_update(model, tf_dataset, server_weights, client_optimizer)\n",
    "\n",
    "  @tff.tf_computation(model_weights_type)\n",
    "  def server_update_fn(mean_client_weights):\n",
    "    model = model_fn()\n",
    "    return server_update(model, mean_client_weights)\n",
    "\n",
    "  @tff.federated_computation\n",
    "  def initialize_fn():\n",
    "    return tff.federated_value(server_init(), tff.SERVER)\n",
    "\n",
    "\n",
    "  @tff.federated_computation(federated_server_type, federated_dataset_type)\n",
    "  def next_fn(server_weights, federated_dataset):\n",
    "\n",
    "    # Broadcast the server weights to the clients.\n",
    "    server_weights_at_client = tff.federated_broadcast(server_weights)\n",
    "\n",
    "    # Each client computes their updated weights.\n",
    "    client_weights = tff.federated_map(\n",
    "        client_update_fn, (federated_dataset, server_weights_at_client))\n",
    "    \n",
    "    # The server averages these updates.\n",
    "    mean_client_weights = tff.federated_mean(client_weights)\n",
    "\n",
    "    # # The server updates its model.\n",
    "    server_weights = tff.federated_map(server_update_fn, mean_client_weights)\n",
    "    return server_weights\n",
    "\n",
    "  def evaluate(server_state, train_data, test_data, trainable_ids):\n",
    "    server_weights = []\n",
    "    acc_mean, loss_mean, k_acc_mean = [], [], []\n",
    "    model = create_keras_bn_model(test_data[0].element_spec[0].shape[1])\n",
    "    model.compile(\n",
    "          loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "          metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "                  tf.keras.metrics.SparseTopKCategoricalAccuracy(k=k)]  \n",
    "    )\n",
    "    weights = model.get_weights()\n",
    "\n",
    "    for idx, weight_id in enumerate(trainable_ids):\n",
    "      weights[weight_id] = np.array(server_state[idx])\n",
    "      \n",
    "    model.set_weights(weights)\n",
    "    print(\"\\t--Training--\\t\")\n",
    "    for batch in train_data:\n",
    "      loss, acc, k_acc = model.evaluate(batch, batch_size=BATCH_SIZE, verbose=1)\n",
    "      loss_mean.append(loss)\n",
    "      acc_mean.append(acc)\n",
    "      k_acc_mean.append(k_acc)\n",
    "    train_loss = statistics.mean(loss_mean)\n",
    "    train_acc = statistics.mean(acc_mean)\n",
    "    train_k_acc = statistics.mean(k_acc_mean)\n",
    "    acc_mean, loss_mean, k_acc_mean = [], [], []\n",
    "\n",
    "    print(\"\\t--Testing--\\t\")\n",
    "    for batch in test_data:\n",
    "      loss, acc, top_k_acc = model.evaluate(batch, batch_size=BATCH_SIZE, verbose=1)\n",
    "      loss_mean.append(loss)\n",
    "      acc_mean.append(acc)\n",
    "      k_acc_mean.append(k_acc)\n",
    "    test_loss = statistics.mean(loss_mean)\n",
    "    test_acc = statistics.mean(acc_mean)\n",
    "    test_k_acc = statistics.mean(k_acc_mean)\n",
    "    return {\"Train_acc\":train_acc, f\"Train_{k}_acc\":train_k_acc, \"Train_loss\":train_loss, \"Test_acc\":test_acc, f\"Test_{k}_acc\":test_k_acc,  \"Test_loss\":test_loss,}\n",
    "    # -------------------END---------------------\n",
    "\n",
    "  with tf.device('/gpu:0'):\n",
    "    if use_bn:    #if batch normalization dont update the clients weights of the batch normalization layer. \n",
    "        #returns just server state-> \n",
    "        trainable_ids = map_weights_ids()\n",
    "        fed_avg = tff.templates.IterativeProcess(\n",
    "                              initialize_fn=initialize_fn,\n",
    "                              next_fn=next_fn\n",
    "                              )\n",
    "        \n",
    "        state = fed_avg.initialize()\n",
    "        with summary_writer.as_default():\n",
    "            for round_num in range(EPOCHS):  \n",
    "                state = fed_avg.next(state, train_data)\n",
    "                metrics = evaluate(state, train_data, test_data, trainable_ids)\n",
    "                for name, value in metrics.items():\n",
    "                  tf.summary.scalar(name, value, step=round_num)\n",
    "                  print(round_num, name, value)\n",
    "              \n",
    "    else:\n",
    "      fed_avg = tff.learning.build_federated_averaging_process(\n",
    "          model_fn,\n",
    "          client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= learning_rate, momentum=momentum, nesterov=nesterov), \n",
    "          server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate= 1.0)\n",
    "          )\n",
    "\n",
    "      state = fed_avg.initialize()\n",
    "      with summary_writer.as_default():\n",
    "          for round_num in range(EPOCHS):\n",
    "              state, metrics = fed_avg.next(state, train_data)\n",
    "              # Note: training metrics reported by the iterative training process \n",
    "              #generally reflect the performance of the model at the beginning of the training round\n",
    "              for name, value in metrics['train'].items():\n",
    "                  tf.summary.scalar(name, value, step=round_num)\n",
    "                  print(round_num, name, value)\n",
    "                    \n",
    "              evaluation = tff.learning.build_federated_evaluation(model_fn)  \n",
    "              test_metrics = evaluation(state.model, test_data)\n",
    "              for name, value in test_metrics.items():\n",
    "                  tf.summary.scalar(name, value, step=round_num)\n",
    "                  print(round_num, name, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77RS9R1uU1WY"
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(tf_log_dir + \"/\" + file + \"-\" + str(CLIENTS) + \"/federated-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "run_federated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRXvHm1t3NJO"
   },
   "source": [
    "## Unfederated Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dX2J_ediZLmY"
   },
   "outputs": [],
   "source": [
    "def run_unfederated(ds_train, ds_test, ds_val, input_dim):\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                min_delta=0.01, \n",
    "                                patience=2, \n",
    "                                verbose=1, \n",
    "                                mode='auto', \n",
    "                                baseline=None, \n",
    "                                restore_best_weights=True)\n",
    "    if use_bn:\n",
    "      model = BNModel(NUM_CLASSES)\n",
    "    else:\n",
    "      model = FLModel(NUM_CLASSES)\n",
    "    model.compile(\n",
    "                optimizer= client_optimizer, \n",
    "                loss= \"sparse_categorical_crossentropy\", \n",
    "                metrics= [\n",
    "                          sparseCategoricalAcc, \n",
    "                          sparseTopKCategoricalAccuracy\n",
    "                          ]\n",
    "                )\n",
    "    for epoch in range(EPOCHS):\n",
    "        with tf.device('/gpu:0'):\n",
    "            history = model.fit(\n",
    "                              ds_train,\n",
    "                              steps_per_epoch=64, \n",
    "                              validation_data = ds_val, \n",
    "                              verbose=0,\n",
    "                              callbacks = [early_stopping_callback])\n",
    "\n",
    "        loss = round(history.history[\"loss\"][0], 8)\n",
    "        acc = round(history.history[\"sparse_categorical_accuracy\"][0], 8)\n",
    "        k_acc =  round(history.history[\"sparse_top_k_categorical_accuracy\"][0], 8)\n",
    "        val_loss =  round(history.history['val_loss'][0], 8)\n",
    "        val_acc = round(history.history['val_sparse_categorical_accuracy'][0], 8)\n",
    "        val_k_acc =  round(history.history['val_sparse_top_k_categorical_accuracy'][0], 8)\n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "            test_loss, test_acc, test_k_acc = model.evaluate(ds_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "        \n",
    "        test_loss = round(test_loss, 8)\n",
    "        test_acc = round(test_acc, 8)\n",
    "        test_k_acc = round(test_k_acc, 8)\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Loss/train\", loss, step=epoch)\n",
    "            tf.summary.scalar(\"Acc/train\", acc, step=epoch)\n",
    "            tf.summary.scalar(\"K_acc/train\", k_acc, step=epoch)\n",
    "\n",
    "            tf.summary.scalar(\"Loss/validation\", val_loss, step=epoch)\n",
    "            tf.summary.scalar(\"Acc/validation\", val_acc, step=epoch)\n",
    "            tf.summary.scalar(\"K_acc/validation\", val_k_acc, step=epoch)\n",
    "\n",
    "            tf.summary.scalar(\"Loss/test\", test_loss, step=epoch)\n",
    "            tf.summary.scalar(\"Acc/test\", test_acc, step=epoch)\n",
    "            tf.summary.scalar(\"K_acc/test\", test_k_acc, step=epoch)\n",
    "\n",
    "        print(\n",
    "          f'Epoch: {epoch},\\n'\n",
    "          f'Train Loss:\\t{loss}, '\n",
    "          f'Train Accuracy:\\t{acc}, '\n",
    "          f'Train Top 5 Accuracy:\\t{k_acc}\\n'\n",
    "          f'Validation Loss:\\t{val_loss}, '\n",
    "          f'Validation Accuracy:\\t{val_acc}, '\n",
    "          f'Validation Top 5 Accuracy:\\t{val_k_acc}\\n'\n",
    "          f'Test Loss:\\t{test_loss}, '\n",
    "          f'Test Accuracy:\\t{test_acc} '\n",
    "          f'Test Top 5 Accuracy:\\t{test_k_acc}'\n",
    "          f'\\n--------------------------------------------------------------------------------------------------------------------------\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ag6lU8AFOw6m"
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(tf_log_dir + \"/\" + file + \"-\" + str(CLIENTS) + \"/unfederated-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#get same client ids, as with tff\n",
    "mask = np.isin(data[:, 0], client_ids)\n",
    "x = data[mask].copy() \n",
    "\n",
    "rununfederated_train, unfederated_test, unfederated_val = create_unfederated_dataset(x, reader.get_features())\n",
    "_unfederated(unfederated_train, unfederated_test, unfederated_val,  (reader.get_features()-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNT_g99NVyBm"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {tf_log_dir}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FL_vs_Non_FL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}